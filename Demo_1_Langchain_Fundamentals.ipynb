{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "05b268f9",
   "metadata": {},
   "source": [
    "# ðŸ¦œðŸ”— LangChain Fundamentals: Interactive Demo\n",
    "\n",
    "Welcome to this hands-on demonstration!  \n",
    "\n",
    "This notebook provides a comprehensive, interactive demonstration of the **core components that make LangChain powerful for building production-ready AI applications**.  \n",
    "\n",
    "You'll explore how to communicate with AI models, enable them to use tools, and make them work with your own data through practical, real-world examples.\n",
    "\n",
    "## ðŸ“š What This Demo Covers\n",
    "\n",
    "### **1. Prompt Templates**\n",
    "Learn how to communicate with AI models efficiently using reusable and structured prompts that ensure consistent behavior.\n",
    "\n",
    "### **2. Chat Models**\n",
    "These are the *brains* of your applicationâ€”Large Language Models (LLMs) that generate responses, reason through problems, and follow complex instructions.\n",
    "\n",
    "### **3. LCEL (LangChain Expression Language)**\n",
    "Connect multiple components together to build sophisticated workflows and data processing pipelines.  \n",
    "\n",
    "### **4. Structured Output**\n",
    "Receive clean, typed outputs (like JSON or Pydantic models) for seamless integration with your application code.\n",
    "\n",
    "### **5. Tool Calling**\n",
    "Enable AI models to use external toolsâ€”calculators, APIs, databases, or any custom function you provide.\n",
    "\n",
    "### **6. RAG (Retrieval Augmented Generation)**\n",
    "Provide external knowledge sources (PDFs, documents, databases, web pages) so the AI can *answer questions based on your specific data*.\n",
    "\n",
    "---\n",
    "\n",
    "**Each section includes:**\n",
    "- Clear explanations of the concept\n",
    "- Real-world analogies to make complex ideas intuitive\n",
    "- Working code examples you can run and modify\n",
    "- Technical context to understand what's happening under the hood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe96616f",
   "metadata": {},
   "source": [
    "## Setup: API Keys\n",
    "\n",
    "**For Local/Jupyter:** Set your OpenAI key in a `.env` file (skip if already in your shell env).\n",
    "\n",
    "**For Google Colab:** Use Colab's secrets manager:\n",
    "1. Click the ðŸ”‘ key icon in the left sidebar\n",
    "2. Add a new secret with name `OPENAI_API_KEY` and your API key as the value\n",
    "3. The code below will automatically detect and use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09c4323f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(dotenv_path=\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e21cfcd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook kernel executable: /opt/anaconda3/bin/python\n",
      "Working directory: /Users/massadraza/Desktop/Langchain Prep/langchain-langgraph-demo\n",
      "sys.path (truncated): ['/opt/anaconda3/lib/python313.zip', '/opt/anaconda3/lib/python3.13', '/opt/anaconda3/lib/python3.13/lib-dynload', '', '/opt/anaconda3/lib/python3.13/site-packages', '/opt/anaconda3/lib/python3.13/site-packages/aeosa']\n",
      "langchain_core installed at: /opt/anaconda3/lib/python3.13/site-packages/langchain_core/__init__.py\n",
      "ChatPromptTemplate import: OK\n",
      "langchain_prompts loaded from: /Users/massadraza/Desktop/Langchain Prep/langchain-langgraph-demo/langchain_prompts.py\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic cell: prints kernel info and tests imports\n",
    "import sys, traceback, os\n",
    "print(\"Notebook kernel executable:\", sys.executable)\n",
    "#print(\"Python version:\", sys.version.replace('\n",
    "#', ' '))\n",
    "print(\"Working directory:\", os.getcwd())\n",
    "print(\"sys.path (truncated):\", sys.path[:10])\n",
    "\n",
    "# Test top-level package import\n",
    "try:\n",
    "    import langchain_core\n",
    "    print(\"langchain_core installed at:\", getattr(langchain_core, '__file__', str(langchain_core)))\n",
    "except Exception:\n",
    "    print(\"Failed to import langchain_core:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test specific symbol import\n",
    "try:\n",
    "    from langchain_core.prompts import ChatPromptTemplate\n",
    "    print(\"ChatPromptTemplate import: OK\")\n",
    "except Exception:\n",
    "    print(\"Failed to import ChatPromptTemplate from langchain_core.prompts:\")\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Also verify local helper module imports that the notebook uses\n",
    "try:\n",
    "    import langchain_prompts\n",
    "    print(\"langchain_prompts loaded from:\", getattr(langchain_prompts, '__file__', str(langchain_prompts)))\n",
    "except Exception:\n",
    "    print(\"Failed to import local module langchain_prompts:\")\n",
    "    traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236d5dd3",
   "metadata": {},
   "source": [
    "## 1) Prompt Templates\n",
    "Think of sending professional emails. You probably have templates like:\n",
    "*\"Dear **{Name}**, I'm writing to inform you about **{Topic}**. Please let me know by **{Deadline}**.\"*\n",
    "\n",
    "You don't compose a brand new email from scratch every timeâ€”you use the same structure and just fill in the blanks with different names, topics, and dates. Prompt Templates work the same mannerâ€”they're reusable blueprints where you swap in dynamic variables without rewriting the entire prompt.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `ChatPromptTemplate`: This is our blueprint.\n",
    "* `{user_query}` and `{todays_date}`: These are the **variables** (the blanks we will fill in later).\n",
    "* We use a System Prompt to tell the AI *who* it is (a Date Assistant) before it answers.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1fdc3238",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[SystemMessage(content=\"\\n\\nYou are a helpful and accurate date assistant. \\nYou are given a user's request, and today's date. Your job is to accurately infer the date in YYYY-MM-DD format from the user's request.\\n\\ntoday's date : 2026-01-07\\n\\nchat_history:\\n\\n\", additional_kwargs={}, response_metadata={}),\n",
       " HumanMessage(content=\"\\nWhat is tomorrow's date?\\n\", additional_kwargs={}, response_metadata={})]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_prompts import DATE_ASSISTANT_SYSTEM_PROMPT\n",
    "from datetime import datetime\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", DATE_ASSISTANT_SYSTEM_PROMPT),\n",
    "    (\"human\", \"{user_query}\")\n",
    "])\n",
    "\n",
    "example_user_query = \"\"\"\n",
    "What is tomorrow's date?\n",
    "\"\"\"\n",
    "\n",
    "messages = prompt.format_messages(user_query = example_user_query, \n",
    "                                    todays_date = datetime.now().strftime(\"%Y-%m-%d\"))  # Shows message objects ready for a chat model\n",
    "\n",
    "messages\n",
    "\n",
    "\n",
    "# Other Message Objects\n",
    "    # System messages\n",
    "    # Human messages\n",
    "    # AIMessage\n",
    "    # ToolMessage\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3883f54",
   "metadata": {},
   "source": [
    "## 2) Chat Models (OpenAI)\n",
    "\n",
    "Think of Chat Models as different brands of TVs (Sony, Samsung, LG). Each one works differently on the inside, but LangChain gives you a **\"Universal Remote\"** to control them all.\n",
    "This \"Universal Remote\" is called the **Runnable Interface**. It means you can press the \"Play\" button (`invoke`) on *any* model, and it will work exactly the same way. You don't need to learn a new remote control just because you bought a new TV.\n",
    "\n",
    "### ðŸ§  Interface Concepts\n",
    "all Chat Models implement the **Runnable Interface**. This guarantees they all speak the same language:\n",
    "1.  **Standard Inputs:** They all accept a list of `Messages` (System, Human, AI).\n",
    "2.  **Standard Outputs:** They all return an `AIMessage`.\n",
    "3.  **Standard Methods:**\n",
    "    * `.invoke()`: Send a message and wait for the full answer.\n",
    "    * `.stream()`: Get the answer word-by-word (like a typewriter).\n",
    "    * `.batch()`: Send 50 questions at once and get 50 answers back.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `init_chat_model`: A universal function to load a model.\n",
    "* `model=\"gpt-4o-mini\"`: We are selecting a specific, fast model from OpenAI.\n",
    "* `temperature=0`: We set this to 0 to make the AI **factual and consistent**. Higher numbers (up to 1) make it more creative and random.\n",
    "\n",
    "- Supports over 100 model providers : https://docs.langchain.com/oss/python/integrations/chat\n",
    "- https://docs.langchain.com/oss/python/langchain/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54e4a7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from langchain_openai import ChatOpenAI\n",
    "except Exception:\n",
    "    try:\n",
    "        from langchain.chat_models import ChatOpenAI\n",
    "    except Exception as e:\n",
    "        raise ImportError(\n",
    "            \"Could not import ChatOpenAI. Install 'langchain-openai' or a recent 'langchain' package.\"\n",
    "        ) from e\n",
    "\n",
    "# Shim for the missing init_chat_model API used elsewhere in the notebook\n",
    "def init_chat_model(model: str = \"gpt-4o-mini\", temperature: float = 0, **kwargs):\n",
    "    return ChatOpenAI(model=model, temperature=temperature, **kwargs)\n",
    "\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0) # Tip: Try other models/model providers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cdd2aeb",
   "metadata": {},
   "source": [
    "## 3) LLM Invocation  \n",
    "\n",
    "* **Invoke:** This is like sending a text message and waiting for the reply. You don't see anything until the whole message arrives.\n",
    "* **Stream:** This is like watching someone type in real-time. You see the words appear one by one. This feels much faster to a user.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `llm.invoke(messages)`: Sends our formatted prompt to the AI and waits for the full `AIMessage` response.\n",
    "* `llm.stream(messages)`: Returns a generator that prints tokens as they arrive.\n",
    "\n",
    "ðŸ“š **[Docs](https://docs.langchain.com/oss/python/langchain/models#invocation)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1da0a154",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='2026-01-08', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 6, 'prompt_tokens': 73, 'total_tokens': 79, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CvWwKckVmXU5UOL3JW1L8z7l2UEuV', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='lc_run--019b9ab8-a667-7ee3-b905-b2d554816e0e-0', tool_calls=[], invalid_tool_calls=[], usage_metadata={'input_tokens': 73, 'output_tokens': 6, 'total_tokens': 79, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Invoke : \n",
    "response = llm.invoke(messages)\n",
    "response\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dc9f181e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-08\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be21802a",
   "metadata": {},
   "source": [
    "## 3.1) Streaming (Real-time Output)\n",
    "\n",
    "Imagine you hired a ghostwriter to write a story.\n",
    "* **Invoke (Standard):** The writer goes into a private room, locks the door, writes the entire story, and comes out 20 minutes later to hand you the finished manuscript. You sit in silence waiting the whole time.\n",
    "* **Stream (Real-time):** The writer sits right in front of you and types. You see every letter appear on the screen the moment they press the key. The story takes the same amount of time to finish, but it **feels instant** because you are engaged immediately.\n",
    "\n",
    "### Technical Concept\n",
    "LLMs don't \"think\" of a whole sentence at once; they generate text **one token (word part) at a time**.\n",
    "* **The Bottleneck:** By default (`.invoke()`), the program buffers (holds) all these tokens in memory until the model is completely finished. This creates **Latency** (the delay between asking and seeing the answer).\n",
    "* **The Solution:** The `.stream()` method bypasses this buffer. It creates a Python **Generator** that yields each token (`AIMessageChunk`) the exact millisecond it is created. This drastically lowers the \"Time to First Token.\"\n",
    "\n",
    "### Code Context\n",
    "In the code below:\n",
    "* `llm.stream(messages)`: This replaces `.invoke()`. It returns an iterable stream, not a final string.\n",
    "* `print(..., end=\"\", flush=True)`: We force Python to print *immediately* without moving to a new line, creating that smooth \"typewriter\" effect.\n",
    "\n",
    "ðŸ“š **[Docs: Streaming](https://docs.langchain.com/oss/python/langchain/streaming)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "005f7300",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow's date is 2026-01-08."
     ]
    }
   ],
   "source": [
    "# Streaming : \n",
    "\n",
    "for chunk in llm.stream(messages):\n",
    "    print(chunk.content, end=\"\", flush=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a35d225",
   "metadata": {},
   "source": [
    "## 4) Chaining with LCEL (LangChain Expression Language)\n",
    "\n",
    "Imagine a factory assembly line:\n",
    "* **Station A:** Puts dough on the belt (The Prompt).\n",
    "* **Station B:** Bakes the dough (The Model).\n",
    "* **Station C:** Packages the bread (The Output Parser).\n",
    "\n",
    "LCEL lets us pipe (`|`) these steps together. The output of one step automatically becomes the input of the next.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `chain = prompt | llm | StrOutputParser()`\n",
    "* The `|` symbol is the magic pipe.\n",
    "* `StrOutputParser`: Converts the complex AI message object directly into a simple string so we don't have to do `print(response.content)`.\n",
    "\n",
    "ðŸ“š **[Docs: LCEL Conceptual Guide]()**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bda41d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tomorrow's date is 2026-01-03.\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "\n",
    "print(chain.invoke({ 'user_query' : example_user_query, \n",
    "                    'todays_date' : datetime.now().strftime(\"%Y-%m-%d\") }))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb46a4c",
   "metadata": {},
   "source": [
    "## 5) Structured Output\n",
    "\n",
    "Usually, AI likes to chat and write paragraphs. But sometimes, you want a form filled out, not a conversation.\n",
    "If you ask for a date range, you don't want *\"Sure! The start date is...\"*. You want `{\"start\": \"2025-01-01\", \"end\": \"2025-01-05\"}`.\n",
    "Structured output forces the AI to stop chatting and generate data matching a specific schema.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `class DateRange(BaseModel)`: We define the \"Form\" we want the AI to fill out using Pydantic.\n",
    "* `llm.with_structured_output(DateRange)`: We tell the LLM, \"Your output **must** match this class.\"\n",
    "* The result is an actual Python object, not a string.\n",
    "\n",
    "ðŸ“š **[Docs: Structured Output](https://docs.langchain.com/oss/python/langchain/models#structured-output)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2a6db181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Supports\n",
    "    # - pydantic\n",
    "    # - Typedict\n",
    "    # - Dataclass \n",
    "    # - Json schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e2658e25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DateRange(start_date='2026-01-08', end_date='2026-01-11')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import List, Optional, TypedDict\n",
    "from pydantic import BaseModel, Field\n",
    "from langchain_prompts import DATE_EXTRACTOR_PROMPT_TEMPLATE\n",
    "\n",
    "# You can define structure with Pydantic\n",
    "class DateRange(BaseModel):\n",
    "    start_date: str = Field(default = None, description=\"Start Date of the date range\")\n",
    "    end_date: str = Field(default = None, description=\"End Date of the date range\")\n",
    "\n",
    "# OR you can define structure with TypedDict\n",
    "# class DateRange(TypedDict):\n",
    "#     start_date: str\n",
    "#     end_date: str\n",
    "\n",
    "\n",
    "llm = init_chat_model(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "structured_llm = llm.with_structured_output(DateRange)\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", DATE_EXTRACTOR_PROMPT_TEMPLATE),\n",
    "    (\"human\", \"{user_query}\")\n",
    "])\n",
    "\n",
    "\n",
    "user_query = \"I need time off for the next 4 days\"\n",
    "todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "\n",
    "chain = prompt | structured_llm\n",
    "\n",
    "llm_response = chain.invoke({'todays_date': todays_date, 'user_query': user_query})\n",
    "\n",
    "llm_response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5f8960f",
   "metadata": {},
   "source": [
    "## 6) Tool Calling\n",
    "\n",
    "LLMs are great at writing poetry, but they are often bad at math and they don't know the current weather.\n",
    "Tool calling is like giving the AI **specialized gadgets** to help it do its job.\n",
    "You tell the AI: *\"I am giving you a calculator tool. If you see a math problem, don't guessâ€”use the tool.\"*\n",
    "The AI decides **when** it needs help and **which** gadget to pick to solve the problem.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `@tool`: This is a decorator. It puts a \"sticker\" on your Python function that says *\"Hey AI, you are allowed to use this!\"*\n",
    "* `llm.bind_tools(tools)`: This effectively hands the manual of available tools to the \"Brain.\"\n",
    "* **Important:** When the model uses a tool, it doesn't return a text answer immediately. It returns a `tool_call` (a request to run the code). You (the code) must then run that function and give the answer back to the AI.\n",
    "\n",
    "ðŸ“š **[Docs: Tool Calling](https://docs.langchain.com/oss/python/langchain/models#tool-calling)**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "8d418671",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output Type <class 'langchain_core.messages.ai.AIMessage'>\n",
      "Model output content='' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 62, 'total_tokens': 76, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_provider': 'openai', 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_c4585b5b9c', 'id': 'chatcmpl-CvXxWlSfceAZ32wnXRYNx81CnmKlM', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None} id='lc_run--019b9af4-6fde-70c2-a058-9a14ba0d53c4-0' tool_calls=[{'name': 'circle_area', 'args': {'radius': 3}, 'id': 'call_5a82gzLMw3qN7cZQw20u8ulb', 'type': 'tool_call'}] invalid_tool_calls=[] usage_metadata={'input_tokens': 62, 'output_tokens': 14, 'total_tokens': 76, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}\n",
      "Tool calls (if any): [{'name': 'circle_area', 'args': {'radius': 3}, 'id': 'call_5a82gzLMw3qN7cZQw20u8ulb', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain_core.messages import HumanMessage\n",
    "import math\n",
    "\n",
    "@tool\n",
    "def circle_area(radius: float) -> float: \n",
    "    \"\"\"Return the area of a circle for a given radius.\"\"\"\n",
    "    return math.pi * radius * radius\n",
    "\n",
    "tools = [circle_area]\n",
    "\n",
    "# Bind tools to the model so it can decide to call them\n",
    "llm_with_tools = llm.bind_tools(tools)\n",
    "\n",
    "msg = HumanMessage(content=\"I have a circular garden of radius 3. What's the area?\")\n",
    "ai_msg = llm_with_tools.invoke([msg])\n",
    "\n",
    "print(\"Model output Type\", type(ai_msg))\n",
    "print(\"Model output\", ai_msg)\n",
    "print(\"Tool calls (if any):\", getattr(ai_msg, \"tool_calls\", None))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008bbc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final: The area is approximately 28.27 square units.\n"
     ]
    }
   ],
   "source": [
    "# If a tool call is present, execute it and return a final answer:\n",
    "\n",
    "final = None\n",
    "\n",
    "if getattr(ai_msg, \"tool_calls\", None):\n",
    "    for tool_call in ai_msg.tool_calls:\n",
    "        if tool_call[\"name\"] == \"circle_area\":\n",
    "            r = float(tool_call[\"args\"][\"radius\"])\n",
    "            result = circle_area.invoke({\"radius\": r})\n",
    "            final = f\"The area is approximately {result:.2f} square units.\"\n",
    "else:\n",
    "    final = ai_msg.content\n",
    "\n",
    "print(\"Final:\", final)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1ba47ae",
   "metadata": {},
   "source": [
    "## 6.1) Tool Calling - MCP (Model Context Protocol)\n",
    "\n",
    "**Note:** MCP requires Python 3.10+. If you're using Python 3.9, skip this section - you've already learned the core concept of tool calling above!\n",
    "\n",
    "Standard tool calling (above) works great for tools defined in your own code. But what if you want to use a tool living on a different server or a database?\n",
    "**MCP** is like a **Universal USB Standard** for AI tools. It allows your LangChain app to plug into external servers to fetch tools, without you having to write the tool logic yourself.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `MultiServerMCPClient`: Connects to a local script (`mcp_server.py`) acting as a tool provider.\n",
    "* `client.get_tools()`: Automatically fetches the available tools from that server so the LLM can use them.\n",
    "\n",
    "ðŸ“š **[Docs: Model Context Protocol](https://docs.langchain.com/oss/python/langchain/mcp)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8e4348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/w9/06q0g35n6nv7cdc67p24ptr80000gn/T/ipykernel_53605/4190658928.py:13: UserWarning: Could not import MultiServerMCPClient from langchain_mcp_adapters. Falling back to a local stub. Install 'langchain-mcp-adapters' to use real MCP features.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# MCP - Model Context Protocol\n",
    "# This feature works with Python 3.10+\n",
    "\n",
    "# Try real imports first, otherwise provide a lightweight local stub so the notebook can continue.\n",
    "try:\n",
    "    from langchain_mcp_adapters.client import MultiServerMCPClient\n",
    "except Exception:\n",
    "    try:\n",
    "        # alternate import path some installs might expose\n",
    "        from langchain.mcp.adapters.client import MultiServerMCPClient  # type: ignore\n",
    "    except Exception:\n",
    "        import warnings\n",
    "        warnings.warn(\n",
    "            \"Could not import MultiServerMCPClient from langchain_mcp_adapters. \"\n",
    "            \"Falling back to a local stub. Install 'langchain-mcp-adapters' to use real MCP features.\"\n",
    "        )\n",
    "\n",
    "        class MultiServerMCPClient:\n",
    "            \"\"\"\n",
    "            Minimal stub replacement for MultiServerMCPClient used in the notebook.\n",
    "            This stub supports the async get_tools() call and returns an empty list.\n",
    "            Replace with the real implementation by installing `langchain-mcp-adapters`.\n",
    "            \"\"\"\n",
    "\n",
    "            def __init__(self, servers: dict):\n",
    "                self.servers = servers\n",
    "\n",
    "            async def get_tools(self):\n",
    "                # Return an empty tool list so subsequent code can run without MCP.\n",
    "                return []\n",
    "\n",
    "\n",
    "client = MultiServerMCPClient(  \n",
    "    {\n",
    "        \"area\": {\n",
    "            \"transport\": \"stdio\",  # Local subprocess communication\n",
    "            \"command\": \"python\",\n",
    "            \"args\": [\"./mcp_server.py\"]  # Path to your mcp_server.py file\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "tools = await client.get_tools()  \n",
    "\n",
    "tools # You can now bind tools to the model as we did before"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97bc2793",
   "metadata": {},
   "source": [
    "## 7) RAG (Retrieval Augmented Generation)\n",
    "\n",
    "Standard LLMs take tests from memory (and might \"hallucinate\" if they haven't studied). **RAG** allows the AI to take an **\"Open Book\" Test**.\n",
    "\n",
    "1. **Loader:** We buy the textbook (e.g., download the SpaceX Wikipedia page).\n",
    "2. **Splitter:** We rip the pages into small paragraphs so they are easier to handle.\n",
    "3. **Vector Store:** We file these paragraphs in a cabinet organized by *meaning* (numerical vectors).\n",
    "4. **Retriever:** When you ask a question, we find the specific paragraphs relevant to that question and give them to the AI to read before answering.\n",
    "\n",
    "\n",
    "![Diagram](./images/Basic_Rag.png)\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `WebBaseLoader`: A tool to scrape the text from the SpaceX Wikipedia page.\n",
    "* `RecursiveCharacterTextSplitter`: Cuts the long text into chunks of 1000 characters so they fit in the model's context window.\n",
    "* `InMemoryVectorStore`: A temporary database that stores the \"meaning\" of the text chunks.\n",
    "* `rag_chain`: The final pipeline that takes your question â†’ finds docs â†’ sends both to the LLM â†’ gives you the answer.\n",
    "\n",
    "ðŸ“š **Official Documentation:**\n",
    "* [Document Loaders](https://python.langchain.com/docs/integrations/document_loaders/)\n",
    "* [Text Splitters](https://python.langchain.com/docs/concepts/#text-splitters)\n",
    "* [Retrievers](https://python.langchain.com/docs/concepts/#retrievers)\n",
    "* [Vector Stores](https://python.langchain.com/docs/integrations/vectorstores/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a419985a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'langchain_community'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_community\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdocument_loaders\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m WebBaseLoader\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_text_splitters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RecursiveCharacterTextSplitter\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI, OpenAIEmbeddings\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'langchain_community'"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
    "from langchain_prompts import RAG_PROMPT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d32e28dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Wikipedia page...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'WebBaseLoader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 1. Load the SpaceX Wikipedia page using a LangChain document loader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading Wikipedia page...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m loader \u001b[38;5;241m=\u001b[39m WebBaseLoader(\n\u001b[1;32m      4\u001b[0m   web_paths\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttps://en.wikipedia.org/wiki/SpaceX\u001b[39m\u001b[38;5;124m\"\u001b[39m,),\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m docs \u001b[38;5;241m=\u001b[39m loader\u001b[38;5;241m.\u001b[39mload()\n\u001b[1;32m      7\u001b[0m docs\n",
      "\u001b[0;31mNameError\u001b[0m: name 'WebBaseLoader' is not defined"
     ]
    }
   ],
   "source": [
    "  # 1. Load the SpaceX Wikipedia page using a LangChain document loader\n",
    "print(\"Loading Wikipedia page...\")\n",
    "loader = WebBaseLoader(\n",
    "    web_paths=(\"https://en.wikipedia.org/wiki/SpaceX\",),\n",
    ")\n",
    "docs = loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a20f10a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Split docs into smaller chunks\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    ")\n",
    "\n",
    "chunks = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f447794a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['c1dfc55a-f1b5-44f7-9e36-96372ccfbcc4',\n",
       " 'bc258d74-bae7-40a8-a2cd-901724aa6bd7',\n",
       " 'ad9a23a7-be9a-45b9-acad-7728b2565a4a',\n",
       " 'c6fdd12c-b338-47a0-9529-6c7c3cc32641',\n",
       " 'a40d85a5-dbd0-4ce1-9e60-7c2cf61970a2',\n",
       " '5efd361b-154e-4d34-b2e0-590daf6f2d64',\n",
       " '6f906fa9-51f2-4132-a6c2-3b4eff3bf07c',\n",
       " '13d19bcf-9c67-4b8e-8f65-c1a8bfe39e8e',\n",
       " '94da3798-7365-49a9-b592-e1301264d8d5',\n",
       " '318d2dc0-ee1b-44c7-b443-1559968e3a85',\n",
       " 'ad9777a6-b404-41a6-abf3-4a4ce0e1902b',\n",
       " '8a3fd781-3c87-4d32-aeb9-69c3649f4cda',\n",
       " 'bee10e44-09a4-4618-a48a-addac35e4a12',\n",
       " '7ff2187d-cf7e-4e30-935e-e849adf88342',\n",
       " '2266f744-f267-4bfe-89ee-b69b386a75fe',\n",
       " '847c5c63-4391-4e0d-a540-8be0ff0ec0bf',\n",
       " '774304c7-a5ab-4a0e-9d93-9dedad040c7c',\n",
       " '70dc3b65-8df5-4b3c-9bed-3ebdf648c41b',\n",
       " '2dbae181-eb5f-4c98-990d-6aeaec8ac168',\n",
       " 'f1c6054d-4712-451f-a97e-a263a85c060d',\n",
       " '7a06e11e-f7f2-4a72-a1b5-a323cb8462b8',\n",
       " '4f7c381e-d25c-466c-a1e5-48ec06aa313a',\n",
       " '9421a2ab-c1be-4a24-9112-3d09fc8927ce',\n",
       " '8579e684-2cba-452f-b749-acbf1b22453d',\n",
       " 'a92dfb65-1c39-431d-a7ed-5650c3862e83',\n",
       " '6fc8d6ea-2b70-45c5-8bd4-3983ac6fdca7',\n",
       " '78bb5daf-b0ff-4703-ba06-34790c777d01',\n",
       " '65712f86-bd1a-4041-a766-f99a459be9b7',\n",
       " '475923e7-8b0a-45c2-b676-b8ce432be862',\n",
       " '4cd254b0-ef14-4dc3-8320-8564eb86cbe3',\n",
       " '4ff90427-5f23-4522-9b08-716754054aef',\n",
       " '3a9a77e0-0a7f-49c1-a0a9-3d0a3012e34d',\n",
       " '758d994a-3a6a-414c-ab7a-793bef834380',\n",
       " '63dfe6bc-b600-4d06-b4fd-9f76441a8351',\n",
       " '3e987608-a558-4736-855c-16189b96728f',\n",
       " '1b400f5a-1214-4b82-aefb-6d8a6b51f4b4',\n",
       " '0a8fe249-e3a1-4278-8677-a93b9d8734cb',\n",
       " 'fcbd6c02-301e-4260-896e-e971a97d9c18',\n",
       " 'f9691f3b-da48-4ab2-85ef-3e4d5ff024fb',\n",
       " 'd950b9b7-cc5f-497c-b3d6-5aa124e560c3',\n",
       " '4e5f94cb-e6be-4368-b180-f77da8606c64',\n",
       " '0a175a8d-7059-41a1-97b2-d9e634cd7e92',\n",
       " 'b8c5aad9-4029-439e-88d6-1ca4e212bd87',\n",
       " '02d9413b-70e5-4cf3-b003-9307bfe02e38',\n",
       " '5298370a-82f8-4e4a-8a0b-388a2459d5bf',\n",
       " 'fb3e73f5-fbd7-4b2d-8400-e7a2a43346e5',\n",
       " '839e2b2b-f62e-443f-8328-da223eaaf1c6',\n",
       " '0fa18635-5388-4e8d-b94a-0b2100deb04e',\n",
       " 'cd4176da-fe50-4f24-9ee7-6685d22684d0',\n",
       " 'e19f7d0c-2ff8-44ee-8551-d5b13ad3cab6',\n",
       " '1e96e7ab-0a52-4076-8ba4-e4324d7b444a',\n",
       " '21c1283d-c651-4fec-9b2b-6d5e5f71771e',\n",
       " '501ba11b-d280-4f23-b15f-51d2731b1771',\n",
       " '2a9ffc88-22da-4de6-a186-2d88081b01a0',\n",
       " '2104bd64-86b3-4312-bc74-d5df7a53ea91',\n",
       " '494f2a66-5b1d-4b67-abb1-c76a12aebad6',\n",
       " 'ad72e326-f000-4221-875f-bc28a2b01a8d',\n",
       " 'b8ddfde0-8227-45bb-89bb-ca6603814a9e',\n",
       " 'd9218168-c622-49db-b1ef-27f6fb02c49f',\n",
       " '31f0fda5-c670-43fb-9093-127e2d1c0eb4',\n",
       " '33cadaa0-3d2e-46ff-83aa-e227b4b2cbbb',\n",
       " '1058d1e8-2dae-4c9e-a729-af70370efc21',\n",
       " '024b4edd-18c5-49ff-ac80-bf9d6dfc8a4c',\n",
       " '2fd3d93c-5464-4c33-84e8-02d7e860bde6',\n",
       " '53f8a648-f8f3-444d-893e-9c82d03d0c24',\n",
       " '6b783f31-08c9-483f-8b31-97840254cb28',\n",
       " 'd6b309a7-1ff4-4b8c-bff4-af3b6dd9cc09',\n",
       " '1c27875b-238e-4bbf-ac10-817ff9094474',\n",
       " '2b7b9cd5-d17b-44af-bce0-0e6c1dbe292d',\n",
       " 'f2554f94-16a6-4f7d-8e30-8d67c90db226',\n",
       " 'c019d505-1671-4d4a-9a55-c75dcf2e1ccc',\n",
       " '8d2085d8-65aa-4510-bb77-5af52b3f8448',\n",
       " '7bb6d5e6-0b6c-490e-abe8-c921555ae29a',\n",
       " '62af1e63-a649-40e7-9cef-33d4ce1be767',\n",
       " '0cf933d0-aa6a-4e0b-a963-afdecf758d11',\n",
       " '0fd6fd88-3a89-4bca-a79a-072ac1bb1019',\n",
       " '550cfb75-f546-41f3-a10c-3e837dd2eafd',\n",
       " 'c774e2f6-9ab5-4d49-8705-4c9e21c48740',\n",
       " '406e6032-f0f2-4d63-981a-74eb0bb73eab',\n",
       " '96319a7d-db77-450b-8fdf-e31cb12243cd',\n",
       " '630c259f-c31a-406b-a815-9fc9841f000c',\n",
       " '85912345-a413-4d00-b2fb-b45218fa5d7f',\n",
       " '111fb5d8-7585-40b8-9d7f-4b53291ce3c7',\n",
       " '7245c670-e2e4-49ea-8f8f-299147b323d3',\n",
       " 'f4665620-6085-456a-bc7a-a78abce4cecd',\n",
       " '50bad7ae-a348-4bfc-84dc-3288eb3f3ed6',\n",
       " '4ebdc7c8-fb71-4eb2-b55e-dd648d949919',\n",
       " '6c542ee5-2c7a-404c-8b14-4abcbda5fa3a',\n",
       " 'af570afc-c6e3-4609-9a17-c382ae6deb03',\n",
       " '75e1f452-6a90-4c52-b524-e3f96d604976',\n",
       " '6fd5551a-0410-48ca-aac4-9863ff8b85fe',\n",
       " '431aa9ec-9cc8-4f93-8f8e-5760367e50aa',\n",
       " 'd3e2c7b5-e9a7-40c7-b7a8-8e1c25971865',\n",
       " '3035307c-84ce-4966-88c3-ffd614450ec1',\n",
       " '83461869-d31f-42cb-b90b-bc716a35fa2f',\n",
       " 'a54ea840-ca2d-49c4-8698-4a0e981ed10d',\n",
       " 'ffb3b281-bfed-40ce-9b07-9d481aec3770',\n",
       " '58c33fff-0adf-4dcd-872a-aac34f91e785',\n",
       " 'aa14011d-3542-42ed-bb12-52d0c91e67cc',\n",
       " '0ee3e4d0-80d2-478e-ae1c-d1d388391e85',\n",
       " '3fab0228-cf97-46c0-9383-8ad691f60b97',\n",
       " '1b7d42ac-cb83-45ac-a2e8-b142aef20eba',\n",
       " '8fef2a00-1a08-484b-bab8-dabfa3544c44',\n",
       " '54d68ef2-89c1-4a72-b4c0-771c8c18cf6b',\n",
       " 'bc80cc08-b604-47d1-ac12-95dbba525493',\n",
       " 'f7155132-088a-4df8-91a8-e469598efcc3',\n",
       " '023a3d77-fb1f-47a7-8cfb-13cf1ec0da62',\n",
       " '5bdc4119-0dd9-456f-9335-e01793f5429c',\n",
       " '4c51a127-8381-4811-932b-6fd3e8608986',\n",
       " 'bc8e3dfb-4852-4de5-98ee-d27320dda290',\n",
       " '0fc2079d-6c12-4e80-9d21-954be495689e',\n",
       " 'ce8e0da8-a361-4e09-a1c2-005d89b3c34a',\n",
       " '1d0a7058-53e8-4094-9f0b-ad3570655472',\n",
       " '5a2ab51e-dabb-459b-a55e-0e645e08173a',\n",
       " '3474a78f-b75c-49b4-a919-dad44053144c',\n",
       " '1377cdea-7534-4b69-a1a2-cdd4d28d7ff9',\n",
       " '410b5079-26c6-41a1-af73-61355c95e796',\n",
       " 'fb1c9719-d4b5-4350-b2c0-24875ccd0a6c',\n",
       " '87d0d8a4-c83a-4ec7-93ea-5b4d7a553655',\n",
       " '88a7d583-68c5-41f2-ae7e-c9f9cf128ca5',\n",
       " '635e2e4c-789c-442e-8380-e4e7aa22a0f6',\n",
       " 'f4d92fad-8f6b-4b79-b69d-728889cd16ff',\n",
       " '6499a30b-c1dc-409f-876f-f8658577256c',\n",
       " '186ac063-7b59-486d-9a6a-49411ee4961d',\n",
       " 'f50d3039-9c29-4ead-91e1-51007fa40adf',\n",
       " 'e93a84b2-95c6-4a39-a3e2-c37409fc06a8',\n",
       " '8dc919d2-2a7e-4cb2-923f-741cebe288d8',\n",
       " '98df3d05-4d3f-4bd3-9532-74e6864d8a63',\n",
       " '6574d781-e3a2-4519-bd6a-4e7632e61dfa',\n",
       " '7b0e24da-27f3-460a-990f-d6202c97fd6d',\n",
       " '19b0dba4-0e45-417d-882b-efdf80e7fe43',\n",
       " '084fcaed-cde3-403f-b983-f6279f67006f',\n",
       " '314123e4-0086-4746-b925-68e3ded7f9a7',\n",
       " '1d6fd539-3551-4c26-8b6b-21b780f03811',\n",
       " '00b6369e-209a-4346-8a70-2e757db7d870',\n",
       " 'd7a08810-f4eb-4f11-a55d-71e0bd7275c1',\n",
       " 'fb5f8f9e-9f06-46de-9da8-189e0ba368a1',\n",
       " '890d4002-064a-45df-90ae-3e2c428135a4',\n",
       " 'b81bc497-ec39-4ceb-a8e9-2b623e0b4fd7',\n",
       " 'e98a8cd8-ccd8-4215-83fc-65bf11912051',\n",
       " '68ceed0f-c675-403d-80d2-b2f71db83531',\n",
       " '000c12d9-e0dd-43b0-887f-9bfbc56c6cd6',\n",
       " '80e9b02e-097f-4a36-9049-dea80104e329',\n",
       " '7372dc31-7499-40b6-85f2-aefb4dc01486',\n",
       " 'f33e0040-673a-4a68-a005-57dee08bc507',\n",
       " '35357240-c134-4b35-ac36-694854266328',\n",
       " 'a8536e90-aa69-4fad-99fc-9f2f863d16fe',\n",
       " '4f978350-e800-40cf-b5ce-998048df3579',\n",
       " '08102a5c-bc97-47a4-b33e-90b01d830ce3',\n",
       " '385d1e76-1c7a-4a84-8c30-79bcef1db31f',\n",
       " 'bae3ba69-a34c-4238-ba16-6b60872f7b2f',\n",
       " '44f9a983-8f5a-40d9-bb45-ec26f43e5b00',\n",
       " '99ba69c0-2f6b-433b-99ce-694d34569b0a',\n",
       " 'a22f831a-2f22-4994-8c9a-02c867582828',\n",
       " 'd9a8bb47-5b30-4c69-b715-c457caf1e6b4',\n",
       " '07999048-2a54-460b-83b2-4b1aa8232617',\n",
       " '63736382-ffcb-4fe0-a496-456f4e2e02f5',\n",
       " '320485a1-df5f-4ded-9e31-8438e166b34d',\n",
       " '2bce7ed2-7ac3-44ee-a452-c494bd423dd7',\n",
       " 'e11a22c8-24f7-40d1-bdc9-8d399202f692',\n",
       " '0a188d1c-cd4e-47db-9cca-ef49e2d3d152',\n",
       " 'c4c3ffac-7bea-4a57-b95f-43464e638fa0',\n",
       " 'ebb453a6-4776-4660-922e-a52695897425',\n",
       " 'fd9e981a-1539-4aa9-9978-fb60c9611374',\n",
       " '281cd71f-9daa-44ca-b13a-95936cd6c7fb',\n",
       " '533a7b68-8c1b-488b-8c02-4ed758d7b630',\n",
       " 'b637a615-8c25-4d05-ba01-2cc1ca03bb31',\n",
       " '7c6f5dbf-ec56-4f0d-ae15-c51e3872a550',\n",
       " 'a709e289-62d4-461e-ac19-11c801c46b38',\n",
       " 'b413a276-70db-498c-8ea5-b2dcc5d7a175',\n",
       " 'a87f77e8-c58a-445d-a61d-c107302265ae',\n",
       " '13621eb2-83fc-4535-b4eb-c0cb17e8392f',\n",
       " '43c92080-d679-4097-b997-185d5dac912d',\n",
       " '1f0581a1-b22e-4554-b344-b987c3a1c92d',\n",
       " '2e4e8f56-59a6-4eea-a239-487831f01b4a',\n",
       " '16d76629-8013-4a91-a08a-6e810539f7e7',\n",
       " '7fdb7b9c-3e57-4ab6-a345-8e5db54d50b7',\n",
       " 'aadb024e-d9ef-4ffd-b61a-5b1406de24ae',\n",
       " '8c7fd432-1daf-41c4-b354-68086a6aad32',\n",
       " '98e710f6-3fd6-45d3-8458-680e2b008d3f',\n",
       " 'c4bdf0a1-f70f-4b0f-afd4-113b1eac7703',\n",
       " '1df7d674-a089-4c1f-b3c1-a0c37d305ac9',\n",
       " '89c43b65-476e-4010-b872-fe9e0f4fbdb3',\n",
       " 'a3be82e3-f2a5-47ae-806a-dbb6d85a1045',\n",
       " '45a16f01-3acf-41ba-92d1-d7a434776c3b',\n",
       " 'd0851542-d418-47ca-b9d9-b724006c2302',\n",
       " 'e75bf3ea-35b8-4512-8477-20d5867ea862',\n",
       " 'bd87f2ab-7953-42b7-809f-58089bc95f78',\n",
       " '519e3155-203a-4b24-bede-1cc2a09ad5fe',\n",
       " '5c6c1278-1ece-4472-b102-60429c95f816',\n",
       " 'cf2ab151-bba0-41a1-a856-d8b1d20e6f34',\n",
       " '5b3a9af5-aff4-47c1-b7b0-66646c019e7b',\n",
       " 'de084a69-8280-4e5d-b6a6-3ff6de11cb2e',\n",
       " '14db0bcb-0a73-4793-8618-1824f357fa3d',\n",
       " '4ab9429b-887e-40ce-9dc3-2459f0bdb27c',\n",
       " 'fba6f118-e662-4943-8fb5-db230003932b',\n",
       " 'b0afba3b-276f-47cf-ba11-5bc76154d827',\n",
       " 'afaafbcf-7bb1-4168-a4c4-f66da95f4998',\n",
       " '92890909-eba1-47b1-bfd8-834a60e6bf20',\n",
       " '677b05be-eb4d-42f2-9784-ab35b6daaf06',\n",
       " 'f7232ff2-c5a8-4c23-ad17-b8a2a7acbea7',\n",
       " 'd1e4edd3-ff9d-4e99-9514-8d4865aa0f95',\n",
       " 'f073a331-b62a-44fb-a0ec-69142e290821',\n",
       " '4d16daeb-bcf4-461d-ad82-303f28e409aa']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Create an in-memory vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vector_store = InMemoryVectorStore(embeddings)\n",
    "\n",
    "# Add the chunks to the in-memory store\n",
    "vector_store.add_documents(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e898b02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Define a retreiven for the vector store (top-k)\n",
    "retriever = vector_store.as_retriever(\n",
    "    search_kwargs={\"k\": 5}\n",
    "    )\n",
    "\n",
    "# Helper to join document contents into a single string\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "9ae6696f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(id='1b400f5a-1214-4b82-aefb-6d8a6b51f4b4', metadata={'source': 'https://en.wikipedia.org/wiki/SpaceX', 'title': 'SpaceX - Wikipedia', 'language': 'en'}, page_content=\"SpaceX's 2024 Polaris Dawn mission featured the first-ever private spacewalk, marking a major milestone in commercial space exploration.[102]\\nIn 2025, ProPublica reported that Chinese investors were investing in SpaceX via offshore entities, such as the Cayman Islands. Experts speculated that this might raise national security concerns with regulators.[103] Later in 2025 ProPublica reported that Chinese equity ownership in SpaceX likely extended to direct investment, citing the court testimony of investor Iqbaljit Kahlon.[104]\\nBy July 2025, as part of $5 billion equity raise SpaceX agreed to invest $2 billion in xAI.[105]\"),\n",
       " Document(id='5efd361b-154e-4d34-b2e0-590daf6f2d64', metadata={'source': 'https://en.wikipedia.org/wiki/SpaceX', 'title': 'SpaceX - Wikipedia', 'language': 'en'}, page_content='Space Exploration Technologies Corp.Company headquarters, SpaceX Starbase in Starbase, TexasTrade nameSpaceXCompany typePrivateIndustrySpaceTelecommunicationsFoundedMarch\\xa014, 2002 (23 years ago)\\xa0(2002-03-14) in El Segundo, California, U.S.[1]FounderElon MuskHeadquartersSpaceX Starbase, 1 Rocket Road, Starbase, Texas, U.S.Key peopleElon Musk (CEO, chairman & CTO)[2]Gwynne Shotwell (president & COO)[3]ProductsLaunch vehiclesDragon capsulesStarshieldRevenue US$13.1 billion (2024)[4][5]Operating income US$4.5 billion (2024)[6]OwnerElon Musk (42% equity; 79% voting control)[7]Number of employees13,000+[8]\\xa0(September 2023)SubsidiariesStarlink[9]Swarm TechnologiesPioneer AerospaceWebsitespacex.com\\n\\n\\nThis article is part ofa series aboutElon Musk\\n\\n\\nPersonal\\nAwards and honors\\nBusiness career\\nViews\\nFamily\\nInternational relations\\nLegal affairs\\nPublic image\\nTexas Institute of Technology and Science\\nWealth\\nFoundation'),\n",
       " Document(id='3e987608-a558-4736-855c-16189b96728f', metadata={'source': 'https://en.wikipedia.org/wiki/SpaceX', 'title': 'SpaceX - Wikipedia', 'language': 'en'}, page_content='In November 2023, SpaceX announced it would acquire its parachute supplier Pioneer Aerospace out of bankruptcy for $2.2 million.[96][97]\\nOn July 16, 2024, Elon Musk posted on X that SpaceX would move its headquarters from Hawthorne, California, to SpaceX Starbase in Brownsville, Texas. Musk said this was because the recently passed California AB1955 bill \"and the many others that preceded it, attacking both families and companies\".[98] This new law in California bans school districts from requiring that teachers notify parents about changes to a student\\'s sexual orientation and gender identity.[99] The headquarters officially moved to Brownsville, Texas in August 2024, according to records filed with the California Secretary of State.[100] The move to relocate SpaceX\\'s headquarters was seen as largely symbolic, at least in the short term. The Hawthorne facility continues to support the company\\'s Falcon launch vehicles, which was SpaceX\\'s workhorse product in 2024.[101]'),\n",
       " Document(id='13d19bcf-9c67-4b8e-8f65-c1a8bfe39e8e', metadata={'source': 'https://en.wikipedia.org/wiki/SpaceX', 'title': 'SpaceX - Wikipedia', 'language': 'en'}, page_content=\"vte\\nSpace Exploration Technologies Corp., more commonly known as SpaceX, is a private American aerospace company and space transportation company headquartered at the Starbase development site in Starbase, Texas.[10] Since its founding in 2002, the company has made numerous advances in rocket propulsion, reusable launch vehicles, human spaceflight and satellite constellation technology. As of 2025[update], SpaceX is the world's dominant space launch provider, its launch cadence eclipsing all others, including private competitors and national programs like the Chinese space program.[11] SpaceX, NASA, and the United States Armed Forces work closely together by means of governmental contracts.[12]\"),\n",
       " Document(id='a54ea840-ca2d-49c4-8698-4a0e981ed10d', metadata={'source': 'https://en.wikipedia.org/wiki/SpaceX', 'title': 'SpaceX - Wikipedia', 'language': 'en'}, page_content='Antonio Gracias\\n\\nCEO and Chairman of the Investment Committee at Valor Equity Partners[346]\\n\\n\\n2015[347]\\n\\nDonald Harrison\\n\\nPresident of global partnerships and corporate development, Google[348]\\n\\nLeadership changes[edit]\\nIn November 2022, the company announced COO Gwynne Shotwell and vice president Mark Juncosa would oversee Starbase, its Texas launch facility, along with Omead Afshar, who at the time oversaw operations for Tesla in Texas. Shyamal Patel, who was senior director of operations at the site, would shift to its Cape Canaveral site. CNBC reported that these executive moves demonstrated \"the sense of urgency within the company to get Starship flying\".[349][350][351]\\n\\nTaxes[edit]\\nAccording to The New York Times, SpaceX has likely paid little to no federal income taxes due to tax loss deferral of up to $5.4bn in deferred losses, but has privately told investors that it may never realize some or all of those losses.[352]\\n\\nWorkplace culture[edit]')]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = \"when was SapceX founded\"\n",
    "retriever.invoke(user_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "df304784",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. RAG Q/A Chain\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_template(RAG_PROMPT_TEMPLATE)\n",
    "\n",
    "rag_chain = (\n",
    "        {\n",
    "            \"context\": retriever | RunnableLambda(format_docs),\n",
    "            \"question\": RunnablePassthrough(),\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "12cca5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_query = \"when was SapceX founded\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ef919fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SpaceX was founded on March 14, 2002, in El Segundo, California, U.S. The company was established by Elon Musk. Since its inception, SpaceX has made significant advancements in various areas of aerospace technology.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = rag_chain.invoke(user_query)\n",
    "result.content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbb9b425",
   "metadata": {},
   "source": [
    "### ðŸ“š Recommended Resources\n",
    "* **[LangChain Python Tutorials](https://python.langchain.com/docs/tutorials/)**: The official step-by-step guides.\n",
    "* **[LangSmith](https://smith.langchain.com/)**: A tool to trace and debug your chains (vital for production).\n",
    "* **[LangGraph Documentation](https://langchain-ai.github.io/langgraph/)**: The future of building complex agents.\n",
    "* **[LangChain YouTube Channel](https://www.youtube.com/@LangChain)**: Great for visual learners and deep dives into specific topics.\n",
    "* **[Usecases](https://docs.langchain.com/oss/python/learn)**: Explore real-world usecases"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
