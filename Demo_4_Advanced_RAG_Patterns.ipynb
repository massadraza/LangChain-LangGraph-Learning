{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ” Advanced RAG Patterns - Interactive Demo\n",
    "\n",
    "**Welcome to Advanced RAG techniques!**\n",
    "\n",
    "In Notebook 1, we built a basic RAG system that retrieves documents and generates answers. While that works for simple use cases, production RAG systems need more sophisticated techniques to handle:\n",
    "\n",
    "- **Large document collections** (thousands or millions of documents)\n",
    "- **Complex queries** requiring multi-step reasoning\n",
    "- **Poor retrieval results** that need improvement\n",
    "- **Answer quality** and accuracy verification\n",
    "\n",
    "This demo shows you **production-grade RAG patterns** used by leading AI companies.\n",
    "\n",
    "## ðŸ“š What This Demo Covers\n",
    "\n",
    "### **1. Query Rewriting & Expansion**\n",
    "Improve retrieval by reformulating user queries into better search terms.\n",
    "\n",
    "### **2. Hybrid Search (Dense + Sparse)**\n",
    "Combine semantic similarity (embeddings) with keyword matching (BM25) for better results.\n",
    "\n",
    "### **3. Reranking**\n",
    "Use a specialized model to reorder retrieved documents by relevance.\n",
    "\n",
    "### **4. RAG Fusion**\n",
    "Generate multiple query variations, retrieve for each, and combine results.\n",
    "\n",
    "### **5. Contextual Compression**\n",
    "Extract only the relevant portions from retrieved documents to reduce noise.\n",
    "\n",
    "### **6. Self-RAG (Self-Reflective RAG)**\n",
    "Agent evaluates its own answers and decides whether to retrieve more information.\n",
    "\n",
    "### **7. CRAG (Corrective RAG)**\n",
    "Automatically detects poor retrievals and falls back to web search.\n",
    "\n",
    "---\n",
    "\n",
    "**Each pattern includes:**\n",
    "- Why you need it (the problem it solves)\n",
    "- How it works (the technique)\n",
    "- Working code example\n",
    "- When to use it in production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Query Rewriting\n",
    "\n",
    "**The Problem:**\n",
    "Users often ask questions in ambiguous or poorly-worded ways. If you search for exactly what they typed, you might miss relevant documents.\n",
    "\n",
    "**Example:**\n",
    "- User asks: *\"How do I make my app faster?\"*\n",
    "- Better query: *\"application performance optimization techniques\"*\n",
    "\n",
    "**The Solution:**\n",
    "Use an LLM to rewrite the user's query into a more effective search query before retrieving documents.\n",
    "\n",
    "### Code Context\n",
    "We create a chain that:\n",
    "1. Takes the original user query\n",
    "2. Asks the LLM to rewrite it for better retrieval\n",
    "3. Uses the rewritten query to search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Query rewriter\n",
    "query_rewrite_template = \"\"\"You are an expert at reformulating user queries for better document retrieval.\n",
    "\n",
    "Original query: {question}\n",
    "\n",
    "Rewrite this query to be more specific and effective for semantic search. \n",
    "Focus on key concepts and technical terms.\n",
    "\n",
    "Rewritten query:\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "query_rewriter = (\n",
    "    ChatPromptTemplate.from_template(query_rewrite_template)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# Example\n",
    "original_query = \"How do I make my app faster?\"\n",
    "rewritten_query = query_rewriter.invoke({\"question\": original_query})\n",
    "\n",
    "print(f\"Original: {original_query}\")\n",
    "print(f\"Rewritten: {rewritten_query}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Multi-Query Generation (RAG Fusion)\n",
    "\n",
    "**The Problem:**\n",
    "A single query might not capture all aspects of what the user wants. Different phrasings retrieve different documents.\n",
    "\n",
    "**The Solution:**\n",
    "Generate multiple variations of the query, retrieve documents for each, and combine the results with ranking fusion.\n",
    "\n",
    "### Code Context\n",
    "This technique:\n",
    "1. Generates 3-5 different query variations\n",
    "2. Retrieves documents for each variation\n",
    "3. Uses Reciprocal Rank Fusion to combine results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "multi_query_template = \"\"\"You are an AI assistant helping to improve search results.\n",
    "\n",
    "Generate 3 different versions of the given question to retrieve relevant documents from a vector database.\n",
    "Provide these alternative questions separated by newlines.\n",
    "\n",
    "Original question: {question}\n",
    "\n",
    "Alternative questions:\"\"\"\n",
    "\n",
    "multi_query_generator = (\n",
    "    ChatPromptTemplate.from_template(multi_query_template)\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    | (lambda x: x.split(\"\\n\"))\n",
    ")\n",
    "\n",
    "# Example\n",
    "question = \"What are the benefits of using LangChain?\"\n",
    "alternative_queries = multi_query_generator.invoke({\"question\": question})\n",
    "\n",
    "print(\"Generated queries:\")\n",
    "for i, q in enumerate(alternative_queries, 1):\n",
    "    if q.strip():\n",
    "        print(f\"{i}. {q.strip()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Contextual Compression\n",
    "\n",
    "**The Problem:**\n",
    "Retrieved documents are often long and contain irrelevant information. Sending entire documents to the LLM:\n",
    "- Wastes tokens (costs money)\n",
    "- Adds noise (reduces accuracy)\n",
    "- Hits context limits\n",
    "\n",
    "**The Solution:**\n",
    "Use a small, fast model to extract only the relevant portions of each document before sending to the main LLM.\n",
    "\n",
    "### Code Context\n",
    "We use LangChain's `ContextualCompressionRetriever` with an `LLMChainExtractor` to:\n",
    "1. Retrieve documents normally\n",
    "2. Extract only relevant passages\n",
    "3. Return compressed results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain.retrievers.document_compressors import LLMChainExtractor\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "\n",
    "# Load and prepare documents\n",
    "loader = WebBaseLoader(\"https://python.langchain.com/docs/introduction/\")\n",
    "docs = loader.load()\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)\n",
    "splits = splitter.split_documents(docs)\n",
    "\n",
    "# Create vector store\n",
    "embeddings = OpenAIEmbeddings()\n",
    "vectorstore = InMemoryVectorStore(embeddings)\n",
    "vectorstore.add_documents(splits)\n",
    "\n",
    "# Base retriever\n",
    "base_retriever = vectorstore.as_retriever(search_kwargs={\"k\": 5})\n",
    "\n",
    "# Add compression\n",
    "compressor = LLMChainExtractor.from_llm(llm)\n",
    "compression_retriever = ContextualCompressionRetriever(\n",
    "    base_compressor=compressor,\n",
    "    base_retriever=base_retriever\n",
    ")\n",
    "\n",
    "# Test it\n",
    "question = \"What is LangChain?\"\n",
    "compressed_docs = compression_retriever.invoke(question)\n",
    "\n",
    "print(f\"Retrieved {len(compressed_docs)} compressed documents\")\n",
    "print(\"\\nFirst compressed document:\")\n",
    "print(compressed_docs[0].page_content[:300] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Self-Querying Retriever\n",
    "\n",
    "**The Problem:**\n",
    "Users often include filters in their natural language query:\n",
    "- *\"Find articles about Python published after 2023\"*\n",
    "- *\"Show me budget hotels in Paris\"*\n",
    "\n",
    "Basic semantic search doesn't handle these filters well.\n",
    "\n",
    "**The Solution:**\n",
    "Use an LLM to parse the query into:\n",
    "1. Semantic search query (\"Python articles\")\n",
    "2. Metadata filters (year > 2023)\n",
    "\n",
    "### Code Context\n",
    "The self-querying retriever automatically:\n",
    "- Extracts filter criteria from natural language\n",
    "- Applies them to the vector search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers.self_query.base import SelfQueryRetriever\n",
    "from langchain.chains.query_constructor.base import AttributeInfo\n",
    "\n",
    "# Define metadata fields your documents have\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source\",\n",
    "        description=\"The source URL of the document\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"title\",\n",
    "        description=\"The title of the document\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "]\n",
    "\n",
    "document_content_description = \"LangChain documentation and tutorials\"\n",
    "\n",
    "# Create self-querying retriever\n",
    "self_query_retriever = SelfQueryRetriever.from_llm(\n",
    "    llm,\n",
    "    vectorstore,\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Example with implicit filter\n",
    "query = \"What is LangChain? Only show me content from the introduction page\"\n",
    "results = self_query_retriever.invoke(query)\n",
    "\n",
    "print(f\"Found {len(results)} results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Ensemble Retrieval (Hybrid Search)\n",
    "\n",
    "**The Problem:**\n",
    "- **Semantic search** (embeddings) is great for concept matching but can miss exact keyword matches\n",
    "- **Keyword search** (BM25) finds exact terms but misses semantic similarity\n",
    "\n",
    "**Example:**\n",
    "- Query: *\"What is AI?\"*\n",
    "- Semantic search finds \"artificial intelligence, machine learning\" âœ…\n",
    "- Keyword search only finds documents with exact phrase \"What is AI\" âŒ\n",
    "\n",
    "**The Solution:**\n",
    "Combine both approaches and use Reciprocal Rank Fusion to merge results.\n",
    "\n",
    "### Code Context\n",
    "The ensemble retriever:\n",
    "1. Runs both BM25 (keyword) and semantic search in parallel\n",
    "2. Merges results using weighted fusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import EnsembleRetriever\n",
    "from langchain_community.retrievers import BM25Retriever\n",
    "\n",
    "# Create BM25 retriever (keyword-based)\n",
    "bm25_retriever = BM25Retriever.from_documents(splits)\n",
    "bm25_retriever.k = 5\n",
    "\n",
    "# We already have semantic retriever (base_retriever)\n",
    "semantic_retriever = base_retriever\n",
    "\n",
    "# Combine them with ensemble\n",
    "ensemble_retriever = EnsembleRetriever(\n",
    "    retrievers=[bm25_retriever, semantic_retriever],\n",
    "    weights=[0.5, 0.5]  # Equal weight to both\n",
    ")\n",
    "\n",
    "# Test\n",
    "query = \"LangChain agents\"\n",
    "ensemble_results = ensemble_retriever.invoke(query)\n",
    "\n",
    "print(f\"Ensemble retrieval found {len(ensemble_results)} results\")\n",
    "print(\"\\nTop result:\")\n",
    "print(ensemble_results[0].page_content[:200] + \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Parent Document Retriever\n",
    "\n",
    "**The Problem:**\n",
    "When you split documents into small chunks for embedding:\n",
    "- Small chunks are great for precise retrieval\n",
    "- But they lack surrounding context when generating answers\n",
    "\n",
    "**The Solution:**\n",
    "Store small chunks for retrieval, but return the full parent document (or larger chunk) for generation.\n",
    "\n",
    "### Code Context\n",
    "- Index: Small chunks (100-200 tokens)\n",
    "- Retrieve: Find relevant small chunks\n",
    "- Return: The full parent documents for context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.retrievers import ParentDocumentRetriever\n",
    "from langchain.storage import InMemoryStore\n",
    "\n",
    "# Parent splitter (large chunks)\n",
    "parent_splitter = RecursiveCharacterTextSplitter(chunk_size=2000)\n",
    "\n",
    "# Child splitter (small chunks for retrieval)\n",
    "child_splitter = RecursiveCharacterTextSplitter(chunk_size=400)\n",
    "\n",
    "# Store for parent documents\n",
    "store = InMemoryStore()\n",
    "\n",
    "# Create parent document retriever\n",
    "parent_doc_retriever = ParentDocumentRetriever(\n",
    "    vectorstore=vectorstore,\n",
    "    docstore=store,\n",
    "    child_splitter=child_splitter,\n",
    "    parent_splitter=parent_splitter,\n",
    ")\n",
    "\n",
    "# Add documents\n",
    "parent_doc_retriever.add_documents(docs)\n",
    "\n",
    "# Retrieve\n",
    "results = parent_doc_retriever.invoke(\"What is LangChain?\")\n",
    "\n",
    "print(f\"Retrieved {len(results)} parent documents\")\n",
    "print(f\"\\nFirst parent doc length: {len(results[0].page_content)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) CRAG (Corrective RAG)\n",
    "\n",
    "**The Problem:**\n",
    "Sometimes your document collection simply doesn't contain the answer. The retriever returns irrelevant documents, and the LLM generates a hallucinated answer.\n",
    "\n",
    "**The Solution:**\n",
    "Build an agent that:\n",
    "1. Retrieves documents from your knowledge base\n",
    "2. **Evaluates** if they're actually relevant\n",
    "3. If yes â†’ generates answer from documents\n",
    "4. If no â†’ searches the web instead\n",
    "\n",
    "### Code Context\n",
    "We use LangGraph to build a corrective RAG agent with:\n",
    "- Retrieval grading (are docs relevant?)\n",
    "- Web search fallback\n",
    "- Hallucination checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import TypedDict, List\n",
    "from langchain_core.documents import Document\n",
    "from pydantic import BaseModel, Field\n",
    "\n",
    "# Grader for evaluating document relevance\n",
    "class GradeDocuments(BaseModel):\n",
    "    \"\"\"Binary score for relevance check.\"\"\"\n",
    "    binary_score: str = Field(description=\"Documents are relevant to the question, 'yes' or 'no'\")\n",
    "\n",
    "# LLM with structured output for grading\n",
    "llm_grader = llm.with_structured_output(GradeDocuments)\n",
    "\n",
    "grade_prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"You are a grader assessing relevance of a retrieved document to a user question.\n",
    "    \n",
    "Retrieved document:\n",
    "{document}\n",
    "\n",
    "User question:\n",
    "{question}\n",
    "\n",
    "If the document contains keyword(s) or semantic meaning related to the question, grade it as relevant.\n",
    "Give a binary score 'yes' or 'no' to indicate whether the document is relevant.\"\"\"\n",
    ")\n",
    "\n",
    "retrieval_grader = grade_prompt | llm_grader\n",
    "\n",
    "# Example: Grade a document\n",
    "question = \"What is LangChain?\"\n",
    "docs = vectorstore.similarity_search(question, k=1)\n",
    "\n",
    "grade = retrieval_grader.invoke({\n",
    "    \"question\": question,\n",
    "    \"document\": docs[0].page_content\n",
    "})\n",
    "\n",
    "print(f\"Document relevance: {grade.binary_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Complete CRAG Workflow with LangGraph\n",
    "\n",
    "Now let's build the full CRAG system as a graph:\n",
    "\n",
    "```\n",
    "START â†’ Retrieve â†’ Grade Docs â†’ [Relevant?]\n",
    "                                  â†“ Yes: Generate Answer â†’ END\n",
    "                                  â†“ No: Web Search â†’ Generate â†’ END\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import StateGraph, END, START\n",
    "from typing import Annotated\n",
    "import operator\n",
    "\n",
    "# Define the graph state\n",
    "class GraphState(TypedDict):\n",
    "    question: str\n",
    "    documents: List[Document]\n",
    "    generation: str\n",
    "    web_search_needed: bool\n",
    "\n",
    "# Node 1: Retrieve documents\n",
    "def retrieve(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    documents = vectorstore.similarity_search(question, k=3)\n",
    "    return {\"documents\": documents}\n",
    "\n",
    "# Node 2: Grade documents\n",
    "def grade_documents(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    filtered_docs = []\n",
    "    web_search_needed = False\n",
    "    \n",
    "    for doc in documents:\n",
    "        grade = retrieval_grader.invoke({\n",
    "            \"question\": question,\n",
    "            \"document\": doc.page_content\n",
    "        })\n",
    "        \n",
    "        if grade.binary_score == \"yes\":\n",
    "            filtered_docs.append(doc)\n",
    "        else:\n",
    "            web_search_needed = True\n",
    "    \n",
    "    return {\n",
    "        \"documents\": filtered_docs,\n",
    "        \"web_search_needed\": web_search_needed\n",
    "    }\n",
    "\n",
    "# Node 3: Generate answer\n",
    "def generate(state: GraphState) -> GraphState:\n",
    "    question = state[\"question\"]\n",
    "    documents = state[\"documents\"]\n",
    "    \n",
    "    context = \"\\n\\n\".join([doc.page_content for doc in documents])\n",
    "    \n",
    "    prompt = ChatPromptTemplate.from_template(\n",
    "        \"\"\"Answer the question based on the following context:\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Answer:\"\"\"\n",
    "    )\n",
    "    \n",
    "    chain = prompt | llm | StrOutputParser()\n",
    "    generation = chain.invoke({\"context\": context, \"question\": question})\n",
    "    \n",
    "    return {\"generation\": generation}\n",
    "\n",
    "# Node 4: Web search (fallback)\n",
    "def web_search(state: GraphState) -> GraphState:\n",
    "    print(\"âš ï¸ Retrieved documents were not relevant. Would perform web search here.\")\n",
    "    # In production, you'd use a web search tool\n",
    "    # For this demo, we'll just return a message\n",
    "    return {\"documents\": state[\"documents\"]}\n",
    "\n",
    "# Conditional routing\n",
    "def decide_to_generate(state: GraphState) -> str:\n",
    "    if state[\"web_search_needed\"] or len(state[\"documents\"]) == 0:\n",
    "        return \"web_search\"\n",
    "    else:\n",
    "        return \"generate\"\n",
    "\n",
    "# Build the graph\n",
    "workflow = StateGraph(GraphState)\n",
    "\n",
    "# Add nodes\n",
    "workflow.add_node(\"retrieve\", retrieve)\n",
    "workflow.add_node(\"grade_documents\", grade_documents)\n",
    "workflow.add_node(\"generate\", generate)\n",
    "workflow.add_node(\"web_search\", web_search)\n",
    "\n",
    "# Add edges\n",
    "workflow.add_edge(START, \"retrieve\")\n",
    "workflow.add_edge(\"retrieve\", \"grade_documents\")\n",
    "workflow.add_conditional_edges(\n",
    "    \"grade_documents\",\n",
    "    decide_to_generate,\n",
    "    {\n",
    "        \"web_search\": \"web_search\",\n",
    "        \"generate\": \"generate\"\n",
    "    }\n",
    ")\n",
    "workflow.add_edge(\"web_search\", \"generate\")\n",
    "workflow.add_edge(\"generate\", END)\n",
    "\n",
    "# Compile\n",
    "crag_app = workflow.compile()\n",
    "\n",
    "# Test it\n",
    "result = crag_app.invoke({\n",
    "    \"question\": \"What is LangChain?\",\n",
    "    \"documents\": [],\n",
    "    \"generation\": \"\",\n",
    "    \"web_search_needed\": False\n",
    "})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CRAG Answer:\")\n",
    "print(\"=\"*80)\n",
    "print(result[\"generation\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š When to Use Each Pattern\n",
    "\n",
    "| Pattern | Use When | Performance Impact |\n",
    "|---------|----------|-------------------|\n",
    "| **Query Rewriting** | Users ask vague questions | Low |\n",
    "| **Multi-Query (RAG Fusion)** | Need comprehensive results | Medium (multiple retrievals) |\n",
    "| **Contextual Compression** | Documents are long/noisy | Low (saves tokens) |\n",
    "| **Self-Querying** | Users include filters in queries | Low |\n",
    "| **Ensemble/Hybrid** | Need both semantic + keyword matching | Medium |\n",
    "| **Parent Document** | Need context beyond small chunks | Low |\n",
    "| **CRAG** | Knowledge base has gaps | High (grading + potential web search) |\n",
    "\n",
    "## ðŸŽ¯ Production Recommendations\n",
    "\n",
    "**Start with:**\n",
    "1. Basic semantic search (embeddings)\n",
    "2. Add query rewriting for better retrieval\n",
    "3. Add contextual compression to save tokens\n",
    "\n",
    "**Scale up with:**\n",
    "4. Ensemble retrieval (hybrid search)\n",
    "5. Parent document retriever for better context\n",
    "\n",
    "**Advanced production:**\n",
    "6. RAG Fusion for critical queries\n",
    "7. CRAG with web search fallback\n",
    "\n",
    "## ðŸ“š Additional Resources\n",
    "\n",
    "- [LangChain RAG Tutorial](https://python.langchain.com/docs/tutorials/rag/)\n",
    "- [Advanced RAG Techniques](https://blog.langchain.dev/improving-rag/)\n",
    "- [CRAG Paper](https://arxiv.org/abs/2401.15884)\n",
    "- [RAG Evaluation Guide](https://docs.ragas.io/)\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "\n",
    "1. **Basic RAG often isn't enough** for production - you need advanced patterns\n",
    "2. **Query quality matters** - rewriting improves retrieval significantly\n",
    "3. **Combine techniques** - hybrid search + compression + reranking works best\n",
    "4. **Evaluate retrieval** - use CRAG patterns to catch poor results\n",
    "5. **Trade-offs exist** - more sophisticated patterns cost more tokens/latency"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
