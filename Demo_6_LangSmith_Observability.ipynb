{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š LangSmith: Observability & Debugging - Deep Dive\n",
    "\n",
    "**Welcome to LangSmith!**\n",
    "\n",
    "Building LLM applications is easy. Building **production-ready** LLM applications that work reliably is hard.\n",
    "\n",
    "**The Problem:**\n",
    "- Why did the agent choose the wrong tool?\n",
    "- Why is my RAG system returning irrelevant documents?\n",
    "- How much am I spending on OpenAI API calls?\n",
    "- Which prompt version performs better?\n",
    "- Is my app getting slower over time?\n",
    "\n",
    "**The Solution: LangSmith**\n",
    "\n",
    "LangSmith is the **observability platform** for LLM applications. It's like having X-ray vision into your AI system.\n",
    "\n",
    "## ðŸ“š What This Demo Covers\n",
    "\n",
    "### **1. Tracing & Debugging**\n",
    "See every LLM call, tool invocation, and retrieval step in your application.\n",
    "\n",
    "### **2. Evaluation & Testing**\n",
    "Systematically test your LLM application with datasets and metrics.\n",
    "\n",
    "### **3. Monitoring & Analytics**\n",
    "Track costs, latency, and usage patterns in production.\n",
    "\n",
    "### **4. Prompt Engineering**\n",
    "Compare prompt versions and optimize for better results.\n",
    "\n",
    "### **5. Dataset Management**\n",
    "Create and manage test datasets for continuous evaluation.\n",
    "\n",
    "### **6. Feedback & Annotations**\n",
    "Collect user feedback and improve your system iteratively.\n",
    "\n",
    "---\n",
    "\n",
    "**Why you need this:**\n",
    "- **Development**: Debug why agents fail\n",
    "- **Testing**: Ensure quality before deployment  \n",
    "- **Production**: Monitor costs and performance\n",
    "- **Iteration**: Improve based on real usage data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Getting Started with LangSmith\n",
    "\n",
    "### Step 1: Create Account\n",
    "1. Go to [smith.langchain.com](https://smith.langchain.com)\n",
    "2. Sign up (free tier available)\n",
    "3. Create a new project\n",
    "\n",
    "### Step 2: Get API Key\n",
    "1. Go to Settings â†’ API Keys\n",
    "2. Create new API key\n",
    "3. Copy it\n",
    "\n",
    "### Step 3: Configure Environment\n",
    "Add to your `.env` file:\n",
    "```bash\n",
    "LANGSMITH_API_KEY=your_api_key_here\n",
    "LANGSMITH_TRACING=true\n",
    "LANGSMITH_PROJECT=your-project-name\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Enable tracing\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_PROJECT\"] = \"LangChain-Demo\"  # Change to your project name\n",
    "\n",
    "print(\"âœ… LangSmith configured\")\n",
    "print(f\"   Project: {os.environ['LANGSMITH_PROJECT']}\")\n",
    "print(f\"   Tracing: {os.environ['LANGSMITH_TRACING']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Basic Tracing\n",
    "\n",
    "**What is a Trace?**\n",
    "A trace is a complete record of one execution of your application:\n",
    "- What prompt was sent\n",
    "- What the LLM responded\n",
    "- How long it took\n",
    "- How many tokens were used\n",
    "- What tools were called\n",
    "\n",
    "Once tracing is enabled, **every LangChain call is automatically logged** to LangSmith.\n",
    "\n",
    "### Code Context\n",
    "Just run your normal LangChain code - tracing happens automatically!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Create a simple chain\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "prompt = ChatPromptTemplate.from_template(\"Tell me a joke about {topic}\")\n",
    "chain = prompt | llm | StrOutputParser()\n",
    "\n",
    "# This execution will be automatically traced\n",
    "result = chain.invoke({\"topic\": \"programming\"})\n",
    "\n",
    "print(\"Result:\", result)\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ” Check LangSmith UI to see the trace!\")\n",
    "print(\"   Go to: https://smith.langchain.com\")\n",
    "print(\"   You'll see:\")\n",
    "print(\"   - Input prompt\")\n",
    "print(\"   - LLM response\")\n",
    "print(\"   - Token usage\")\n",
    "print(\"   - Latency\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Tracing Complex Workflows\n",
    "\n",
    "**The Power of Traces:**\n",
    "For complex applications (agents, RAG, multi-step chains), traces show the **entire execution tree**:\n",
    "\n",
    "```\n",
    "Main Chain\n",
    "â”œâ”€ Prompt Template\n",
    "â”œâ”€ LLM Call 1\n",
    "â”œâ”€ Tool Call: web_search\n",
    "â”œâ”€ LLM Call 2\n",
    "â””â”€ Output Parser\n",
    "```\n",
    "\n",
    "You can drill into each step and see exactly what happened.\n",
    "\n",
    "### Example: Agent with Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools import tool\n",
    "from langchain.agents import create_agent\n",
    "from datetime import datetime\n",
    "\n",
    "@tool\n",
    "def get_current_time() -> str:\n",
    "    \"\"\"Get the current time.\"\"\"\n",
    "    return datetime.now().strftime(\"%H:%M:%S\")\n",
    "\n",
    "@tool\n",
    "def calculate(expression: str) -> str:\n",
    "    \"\"\"Calculate a mathematical expression.\"\"\"\n",
    "    try:\n",
    "        result = eval(expression)  # Note: unsafe in production\n",
    "        return str(result)\n",
    "    except:\n",
    "        return \"Invalid expression\"\n",
    "\n",
    "tools = [get_current_time, calculate]\n",
    "\n",
    "# Create agent\n",
    "agent = create_agent(\n",
    "    llm,\n",
    "    tools,\n",
    "    system_prompt=\"You are a helpful assistant with access to tools.\"\n",
    ")\n",
    "\n",
    "# Execute with tracing\n",
    "result = agent.invoke({\n",
    "    \"messages\": [{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": \"What time is it? Also calculate 15 * 7\"\n",
    "    }]\n",
    "})\n",
    "\n",
    "print(\"Agent response:\", result[\"messages\"][-1].content)\n",
    "print(\"\\nðŸ” In LangSmith, you'll see:\")\n",
    "print(\"   1. Agent reasoning about which tools to use\")\n",
    "print(\"   2. get_current_time() tool call and result\")\n",
    "print(\"   3. calculate() tool call and result\")\n",
    "print(\"   4. Final response generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Custom Metadata & Tags\n",
    "\n",
    "**The Problem:**\n",
    "In production, you have thousands of traces. How do you find the ones you care about?\n",
    "\n",
    "**The Solution:**\n",
    "Add **tags** and **metadata** to your runs:\n",
    "- Tags: Labels for filtering (\"production\", \"beta\", \"user-123\")\n",
    "- Metadata: Additional context (customer ID, session ID, experiment name)\n",
    "\n",
    "### Code Context\n",
    "Use `RunnableConfig` to add metadata to any chain invocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.runnables import RunnableConfig\n",
    "\n",
    "# Add tags and metadata\n",
    "config = RunnableConfig(\n",
    "    tags=[\"production\", \"joke-generator\", \"v2.0\"],\n",
    "    metadata={\n",
    "        \"user_id\": \"user-123\",\n",
    "        \"session_id\": \"session-456\",\n",
    "        \"experiment\": \"prompt_optimization_v2\",\n",
    "        \"environment\": \"production\"\n",
    "    }\n",
    ")\n",
    "\n",
    "result = chain.invoke(\n",
    "    {\"topic\": \"data science\"},\n",
    "    config=config\n",
    ")\n",
    "\n",
    "print(\"Result:\", result)\n",
    "print(\"\\nðŸ·ï¸ In LangSmith, you can now:\")\n",
    "print(\"   - Filter traces by tag: 'production'\")\n",
    "print(\"   - Search for specific user: 'user-123'\")\n",
    "print(\"   - Compare experiments: 'prompt_optimization_v2'\")\n",
    "print(\"   - Track by environment: 'production' vs 'staging'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4) Evaluation with Datasets\n",
    "\n",
    "**The Problem:**\n",
    "You changed your prompt. Is it better? How do you know?\n",
    "\n",
    "**The Solution:**\n",
    "Create a **test dataset** and run evaluations:\n",
    "1. Define input examples\n",
    "2. Define expected outputs (optional)\n",
    "3. Run your chain on all examples\n",
    "4. Use evaluators to score the results\n",
    "\n",
    "### Creating a Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# Create a dataset\n",
    "dataset_name = \"joke-generation-test\"\n",
    "\n",
    "# Sample inputs for testing\n",
    "examples = [\n",
    "    {\n",
    "        \"inputs\": {\"topic\": \"programming\"},\n",
    "        \"outputs\": {\"expected_contains\": [\"code\", \"bug\", \"developer\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"topic\": \"AI\"},\n",
    "        \"outputs\": {\"expected_contains\": [\"robot\", \"artificial\", \"intelligence\"]}\n",
    "    },\n",
    "    {\n",
    "        \"inputs\": {\"topic\": \"databases\"},\n",
    "        \"outputs\": {\"expected_contains\": [\"SQL\", \"query\", \"table\"]}\n",
    "    },\n",
    "]\n",
    "\n",
    "# Create dataset (only once)\n",
    "try:\n",
    "    dataset = client.create_dataset(dataset_name=dataset_name)\n",
    "    for example in examples:\n",
    "        client.create_example(\n",
    "            inputs=example[\"inputs\"],\n",
    "            outputs=example[\"outputs\"],\n",
    "            dataset_id=dataset.id\n",
    "        )\n",
    "    print(f\"âœ… Created dataset: {dataset_name}\")\n",
    "except:\n",
    "    print(f\"â„¹ï¸  Dataset '{dataset_name}' already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5) Running Evaluations\n",
    "\n",
    "Now run your chain against the entire dataset and evaluate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langsmith.evaluation import evaluate\n",
    "\n",
    "# Define an evaluator\n",
    "def check_joke_quality(run, example):\n",
    "    \"\"\"Check if the joke is relevant to the topic.\"\"\"\n",
    "    output = run.outputs.get(\"output\", \"\")\n",
    "    topic = example.inputs[\"topic\"]\n",
    "    \n",
    "    # Simple check: topic appears in joke\n",
    "    score = 1.0 if topic.lower() in output.lower() else 0.0\n",
    "    \n",
    "    return {\n",
    "        \"key\": \"topic_relevance\",\n",
    "        \"score\": score\n",
    "    }\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate(\n",
    "    lambda inputs: chain.invoke(inputs),\n",
    "    data=dataset_name,\n",
    "    evaluators=[check_joke_quality],\n",
    "    experiment_prefix=\"joke-generation\",\n",
    "    metadata={\"version\": \"v1.0\", \"model\": \"gpt-4o-mini\"}\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ðŸ“Š Evaluation complete!\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nCheck LangSmith UI for:\")\n",
    "print(\"  - Aggregate metrics across all examples\")\n",
    "print(\"  - Individual run comparisons\")\n",
    "print(\"  - Performance over time\")\n",
    "print(\"  - Cost analysis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6) Built-in Evaluators\n",
    "\n",
    "LangSmith provides **pre-built evaluators** for common tasks:\n",
    "\n",
    "### LLM-as-Judge Evaluators\n",
    "Use an LLM to evaluate another LLM's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.evaluation import load_evaluator\n",
    "\n",
    "# Criteria-based evaluator\n",
    "criteria_evaluator = load_evaluator(\n",
    "    \"criteria\",\n",
    "    criteria=\"helpfulness\",  # or \"conciseness\", \"relevance\", etc.\n",
    "    llm=llm\n",
    ")\n",
    "\n",
    "# Test it\n",
    "eval_result = criteria_evaluator.evaluate_strings(\n",
    "    prediction=\"Here's a programming joke: Why do programmers prefer dark mode? Because light attracts bugs!\",\n",
    "    input=\"Tell me a joke about programming\"\n",
    ")\n",
    "\n",
    "print(\"Helpfulness Score:\", eval_result[\"score\"])\n",
    "print(\"Reasoning:\", eval_result[\"reasoning\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7) RAG Evaluation\n",
    "\n",
    "For RAG systems, you need to evaluate:\n",
    "1. **Retrieval quality**: Did we find relevant docs?\n",
    "2. **Answer quality**: Is the answer correct and grounded?\n",
    "3. **Faithfulness**: Does the answer stick to the retrieved context?\n",
    "\n",
    "### Example: RAG Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Context relevance evaluator\n",
    "def evaluate_retrieval(run, example):\n",
    "    \"\"\"Evaluate if retrieved documents are relevant.\"\"\"\n",
    "    # This would check if retrieved docs match the query\n",
    "    # In production, use semantic similarity or LLM-as-judge\n",
    "    return {\"key\": \"retrieval_quality\", \"score\": 0.8}\n",
    "\n",
    "# Answer faithfulness evaluator  \n",
    "def evaluate_faithfulness(run, example):\n",
    "    \"\"\"Check if answer is grounded in retrieved context.\"\"\"\n",
    "    # This would use an LLM to check if answer comes from context\n",
    "    return {\"key\": \"faithfulness\", \"score\": 0.9}\n",
    "\n",
    "\n",
    "print(\"ðŸ“ RAG Evaluation Metrics:\")\n",
    "print(\"\\n1. Context Relevance\")\n",
    "print(\"   Are retrieved documents relevant to the query?\")\n",
    "print(\"\\n2. Answer Faithfulness\")\n",
    "print(\"   Is the answer based on retrieved context?\")\n",
    "print(\"\\n3. Answer Relevance\")\n",
    "print(\"   Does the answer actually address the question?\")\n",
    "print(\"\\n4. Contextual Precision\")\n",
    "print(\"   Are the top-ranked documents the most relevant?\")\n",
    "print(\"\\nRecommended tools:\")\n",
    "print(\"  - RAGAS: ragas.io\")\n",
    "print(\"  - TruLens: trulens.org\")\n",
    "print(\"  - LangChain built-in evaluators\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8) Monitoring Production\n",
    "\n",
    "Once deployed, LangSmith automatically tracks:\n",
    "\n",
    "### Key Metrics\n",
    "- **Latency**: How fast are responses?\n",
    "- **Cost**: How much are you spending?\n",
    "- **Error Rate**: How often do requests fail?\n",
    "- **Token Usage**: Input vs output tokens\n",
    "- **User Feedback**: Thumbs up/down from users\n",
    "\n",
    "### Setting Up Alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"ðŸš¨ Production Monitoring Best Practices:\\n\")\n",
    "print(\"1. Set up alerts in LangSmith UI:\")\n",
    "print(\"   - Latency > 5 seconds\")\n",
    "print(\"   - Error rate > 5%\")\n",
    "print(\"   - Daily cost > $100\")\n",
    "print(\"\\n2. Create dashboards:\")\n",
    "print(\"   - Track metrics over time\")\n",
    "print(\"   - Compare versions (A/B testing)\")\n",
    "print(\"   - Monitor by user segment\")\n",
    "print(\"\\n3. Use feedback:\")\n",
    "print(\"   - Collect thumbs up/down from users\")\n",
    "print(\"   - Use feedback to create training datasets\")\n",
    "print(\"   - Identify failure patterns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9) Collecting User Feedback\n",
    "\n",
    "User feedback is gold for improving your system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simulate a traced run\n",
    "from langsmith import traceable\n",
    "\n",
    "@traceable(name=\"joke_generator\")\n",
    "def generate_joke(topic: str) -> str:\n",
    "    return chain.invoke({\"topic\": topic})\n",
    "\n",
    "# Generate a joke\n",
    "run_id = None  # Will be captured from trace\n",
    "result = generate_joke(\"coffee\")\n",
    "\n",
    "# In your application, you'd capture the run_id from the trace\n",
    "# Then collect user feedback\n",
    "print(\"\\nðŸ’¬ Collecting User Feedback:\")\n",
    "print(\"\\n# In your application:\")\n",
    "print(\"\"\"from langsmith import Client\n",
    "\n",
    "client = Client()\n",
    "\n",
    "# User clicks thumbs up\n",
    "client.create_feedback(\n",
    "    run_id=run_id,\n",
    "    key=\"user_rating\",\n",
    "    score=1.0,  # 1.0 for thumbs up, 0.0 for thumbs down\n",
    "    comment=\"Great joke!\"\n",
    ")\n",
    "\n",
    "# Or custom feedback\n",
    "client.create_feedback(\n",
    "    run_id=run_id,\n",
    "    key=\"relevance\",\n",
    "    score=0.8,\n",
    "    comment=\"Somewhat relevant\"\n",
    ")\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nðŸ“Š Use feedback to:\")\n",
    "print(\"  - Find low-rated outputs\")\n",
    "print(\"  - Create training datasets\")\n",
    "print(\"  - A/B test improvements\")\n",
    "print(\"  - Track user satisfaction over time\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10) Prompt Management & Versioning\n",
    "\n",
    "**The Problem:**\n",
    "Your prompts are scattered across your codebase. When you change one, you don't know what broke.\n",
    "\n",
    "**The Solution:**\n",
    "Store prompts in LangSmith's **Prompt Hub**:\n",
    "- Version control for prompts\n",
    "- A/B test different versions\n",
    "- Rollback to previous versions\n",
    "- Share prompts across team"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "\n",
    "print(\"ðŸŽ¯ Prompt Management Best Practices:\\n\")\n",
    "print(\"1. Store prompts in LangSmith Prompt Hub\")\n",
    "print(\"   ```python\")\n",
    "print(\"   prompt = hub.pull('username/prompt-name')\")\n",
    "print(\"   chain = prompt | llm\")\n",
    "print(\"   ```\")\n",
    "print(\"\\n2. Version your prompts\")\n",
    "print(\"   - v1: Initial version\")\n",
    "print(\"   - v2: Added few-shot examples\")\n",
    "print(\"   - v3: Optimized for conciseness\")\n",
    "print(\"\\n3. Compare versions with evaluations\")\n",
    "print(\"   - Run same dataset on v1, v2, v3\")\n",
    "print(\"   - Compare metrics\")\n",
    "print(\"   - Deploy the winner\")\n",
    "print(\"\\n4. Roll back if needed\")\n",
    "print(\"   - Something broke? Revert to previous version\")\n",
    "print(\"   - No code deployment needed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š LangSmith Features Summary\n",
    "\n",
    "| Feature | Use Case | Value |\n",
    "|---------|----------|-------|\n",
    "| **Tracing** | Debug agent failures | Essential |\n",
    "| **Datasets** | Regression testing | High |\n",
    "| **Evaluations** | Measure quality | High |\n",
    "| **Monitoring** | Track production | Essential |\n",
    "| **Feedback** | Improve over time | High |\n",
    "| **Prompt Hub** | Version control prompts | Medium |\n",
    "| **Annotations** | Label training data | Medium |\n",
    "\n",
    "## ðŸŽ¯ Production Workflow\n",
    "\n",
    "### 1. Development\n",
    "```\n",
    "Write code â†’ Enable tracing â†’ Debug in LangSmith\n",
    "```\n",
    "\n",
    "### 2. Testing\n",
    "```\n",
    "Create dataset â†’ Run evaluations â†’ Compare versions\n",
    "```\n",
    "\n",
    "### 3. Deployment\n",
    "```\n",
    "Deploy â†’ Monitor metrics â†’ Collect feedback\n",
    "```\n",
    "\n",
    "### 4. Iteration\n",
    "```\n",
    "Analyze feedback â†’ Improve prompt â†’ Evaluate â†’ Deploy\n",
    "```\n",
    "\n",
    "## ðŸ’¡ Cost Optimization Tips\n",
    "\n",
    "LangSmith helps you optimize costs by showing:\n",
    "\n",
    "1. **Token usage per request**\n",
    "   - Find requests using excessive tokens\n",
    "   - Optimize context size\n",
    "\n",
    "2. **Expensive chains**\n",
    "   - Which chains cost the most?\n",
    "   - Where to optimize first?\n",
    "\n",
    "3. **Model comparison**\n",
    "   - Test GPT-4 vs GPT-3.5 on your data\n",
    "   - Find the cheapest model that works\n",
    "\n",
    "4. **Caching opportunities**\n",
    "   - Identify repeated queries\n",
    "   - Implement caching\n",
    "\n",
    "## ðŸ”— Additional Resources\n",
    "\n",
    "### Documentation\n",
    "- [LangSmith Docs](https://docs.smith.langchain.com/)\n",
    "- [Evaluation Guide](https://docs.smith.langchain.com/evaluation)\n",
    "- [Tracing Tutorial](https://docs.smith.langchain.com/tracing)\n",
    "\n",
    "### Tutorials\n",
    "- [Testing LLM Applications](https://blog.langchain.dev/testing-llm-applications/)\n",
    "- [Production Monitoring](https://blog.langchain.dev/monitoring-llm-apps/)\n",
    "- [Prompt Engineering](https://blog.langchain.dev/prompt-engineering-guide/)\n",
    "\n",
    "## âœ… Key Takeaways\n",
    "\n",
    "1. **Always enable tracing** during development - it's free debugging\n",
    "2. **Create test datasets** early - prevents regressions\n",
    "3. **Measure before optimizing** - use evaluations to guide decisions\n",
    "4. **Monitor production** - catch issues before users complain\n",
    "5. **Collect feedback** - users tell you what to improve\n",
    "6. **Version your prompts** - enables safe experimentation\n",
    "7. **Track costs** - LLM expenses add up quickly\n",
    "\n",
    "## ðŸš€ Next Steps\n",
    "\n",
    "1. **Sign up** for LangSmith (free tier available)\n",
    "2. **Enable tracing** on your existing projects\n",
    "3. **Create your first dataset** with 10-20 examples\n",
    "4. **Run an evaluation** to get baseline metrics\n",
    "5. **Set up monitoring** for production deploys\n",
    "6. **Collect user feedback** to improve continuously"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
