{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1063e32",
   "metadata": {},
   "source": [
    "# ü¶ú LangChain `create_agent`: High-Level Agent Development - Interactive Demo\n",
    "\n",
    "**Welcome to the high-level agent development demonstration!**\n",
    "\n",
    "In the previous notebook, we built a graph manually (defining nodes, edges, and state) using LangGraph. That approach gave us complete control but required substantial setup and boilerplate code.\n",
    "\n",
    "This notebook introduces **`create_agent`**, a high-level function from the `langchain` module that dramatically simplifies agent creation.\n",
    "\n",
    "## üí° What is `create_agent`?\n",
    "\n",
    "`create_agent` is a **pre-assembled LangGraph** - a production-ready agent template that handles all the common patterns automatically.\n",
    "\n",
    "Under the hood, `create_agent` automatically:\n",
    "1. Creates a `StateGraph` with proper state management\n",
    "2. Adds a model node for reasoning (LLM calls)\n",
    "3. Adds a tool node for executing actions\n",
    "4. Connects them with the correct edges and loops\n",
    "5. Compiles it into a ready-to-use `CompiledGraph`\n",
    "\n",
    "**The result?** You get the same powerful agent capabilities with ~80% less code.\n",
    "\n",
    "## üìö What This Demo Covers\n",
    "\n",
    "### **1. `create_agent` Function**\n",
    "Instantiating a pre-built ReAct (Reasoning + Acting) agent workflow without manually defining nodes and edges.\n",
    "\n",
    "### **2. Standard Interrupts**\n",
    "Handling missing information by pausing agent execution (same pattern as before, but with less code).\n",
    "\n",
    "### **3. Middleware Integration**\n",
    "A powerful way to add cross-cutting concerns to your agent - like human approval steps, logging, monitoring, or validation - without modifying the core agent logic.\n",
    "\n",
    "---\n",
    "\n",
    "## üéØ When to Use `create_agent` vs Manual Graphs\n",
    "\n",
    "**Use `create_agent` when:**\n",
    "- You need a standard ReAct agent (reason ‚Üí act ‚Üí reason ‚Üí act loop)\n",
    "- You want rapid prototyping and development\n",
    "- Your use case fits the common agent pattern\n",
    "- You prefer less boilerplate code\n",
    "\n",
    "**Build manual graphs when:**\n",
    "- You need custom workflow logic beyond ReAct\n",
    "- You require specialized routing or conditional logic\n",
    "- You're implementing novel agent architectures\n",
    "- You need fine-grained control over every step\n",
    "\n",
    "---\n",
    "\n",
    "**This demo shows you:**\n",
    "- How to create production-ready agents in just a few lines\n",
    "- The abstraction layers that LangChain provides\n",
    "- Adding middleware for approval workflows\n",
    "- Understanding what's happening under the hood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c9c2ee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "from langraph_prompts import ACTION_PROMPT_TEMPLATE\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain.chat_models import init_chat_model\n",
    "from langchain.tools import tool\n",
    "from langgraph.checkpoint.memory import InMemorySaver\n",
    "from langgraph.types import Command, interrupt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4bbbd8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Notebook kernel executable: /opt/anaconda3/bin/python\n",
      "Python version: 3.13.5 | packaged by Anaconda, Inc. | (main, Jun 12 2025, 11:23:37) [Clang 14.0.6 ]\n",
      "Working directory: /Users/massadraza/Desktop/Langchain Prep/langchain-langgraph-demo\n",
      "langchain_core installed at: /opt/anaconda3/lib/python3.13/site-packages/langchain_core/__init__.py\n",
      "create_react_agent import: OK\n",
      "langraph_prompts loaded from: /Users/massadraza/Desktop/Langchain Prep/langchain-langgraph-demo/langraph_prompts.py\n"
     ]
    }
   ],
   "source": [
    "# Diagnostic cell: prints kernel info and tests imports\n",
    "import sys, traceback, os\n",
    "print('Notebook kernel executable:', sys.executable)\n",
    "print('Python version:', sys.version.replace('\\n',' '))\n",
    "print('Working directory:', os.getcwd())\n",
    "\n",
    "# Test core package\n",
    "try:\n",
    "    import langchain_core\n",
    "    print('langchain_core installed at:', getattr(langchain_core, '__file__', str(langchain_core)))\n",
    "except Exception:\n",
    "    print('Failed to import langchain_core:' )\n",
    "    traceback.print_exc()\n",
    "\n",
    "# Test specific imports used in this notebook\n",
    "try:\n",
    "    from langgraph.prebuilt import create_react_agent\n",
    "    print('create_react_agent import: OK')\n",
    "except Exception:\n",
    "    print('Failed to import create_react_agent:')\n",
    "    traceback.print_exc()\n",
    "\n",
    "try:\n",
    "    import langraph_prompts\n",
    "    print('langraph_prompts loaded from:', getattr(langraph_prompts, '__file__', str(langraph_prompts)))\n",
    "except Exception:\n",
    "    print('Failed to import langraph_prompts:')\n",
    "    traceback.print_exc()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a809e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tool\n",
    "def get_time_off_balance(user_id: str) -> int:\n",
    "    '''\n",
    "    Get the time off balance for a user. \n",
    "\n",
    "    \n",
    "    Args :\n",
    "        user_id: A unique string representing the user id\n",
    "\n",
    "    Returns:\n",
    "        An integer value representing the time-off balance\n",
    "    '''\n",
    "    return 10\n",
    "\n",
    "@tool\n",
    "def process_time_off(user_id: str, start_date: str, end_date: str) -> dict:\n",
    "    '''\n",
    "    Process time-off request for a user\n",
    "\n",
    "    Args :\n",
    "        user_id: A unique string representing the user id\n",
    "        start_date: A date string in YYYY-MM-DD format for the start date of the time-off request\n",
    "        end_date: A date string in YYYY-MM-DD format for the end date of the time-off request\n",
    "\n",
    "    Returns:\n",
    "        A dictionary with the following keys:\n",
    "        - status: A string value representing the status of the time-off request\n",
    "        - message: A string value representing the message of the time-off request\n",
    "    '''\n",
    "    start_date = datetime.strptime(start_date, \"%Y-%m-%d\")\n",
    "    end_date = datetime.strptime(end_date, \"%Y-%m-%d\")\n",
    "\n",
    "    days_requested = end_date - start_date\n",
    "    time_off_balance = get_time_off_balance.invoke({\"user_id\": user_id})\n",
    "\n",
    "    result = {}\n",
    "\n",
    "    if days_requested.days + 1 > time_off_balance:\n",
    "        result['status'] = 'error'\n",
    "        result['message'] = f'Time off request failed. you only have {time_off_balance} days of remaining leaves but requested {days_requested.days} days'\n",
    "\n",
    "    else:\n",
    "        time_off_balance -= days_requested.days + 1\n",
    "        result['status'] = 'success'\n",
    "        result['message'] = f'Time off request processed successfully for {days_requested.days + 1} days. Your remaining time off balance is {time_off_balance}'\n",
    "\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "@tool\n",
    "def get_additional_info_from_human( message: str) -> str:\n",
    "    '''\n",
    "        Raises an interrupt to fetch additional information from the human requesting the action\n",
    "\n",
    "        Args:\n",
    "            message: a message string from the AI requesting the user for missing information\n",
    "\n",
    "        Returns:\n",
    "            A string value representing the response with additional information from the human\n",
    "\n",
    "    '''\n",
    "    interrupt_result = interrupt(message)\n",
    "\n",
    "    return interrupt_result['user_message']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6120ad",
   "metadata": {},
   "source": [
    "## 1) Creating the Agent (The Easy Way)\n",
    "\n",
    "* **Manual Graph (Previous Notebook):** Like building a PC from scratch. You buy the CPU, RAM, and Motherboard, and connect the wires yourself. It's powerful but takes time.\n",
    "* **`create_agent` (This Notebook):** Like buying a pre-built laptop. You just turn it on. It has the same components inside (LLM, Tools, Memory), but LangChain assembled them for you.\n",
    "\n",
    "###  Code Context\n",
    "In the code below:\n",
    "* `create_agent(...)`: This single function replaces the `StateGraph`, `add_node`, `add_edge`, and `compile` steps.\n",
    "* It automatically creates a graph where the LLM decides to call tools and loops back until it's done.\n",
    "* `checkpointer=InMemorySaver()`: We still need this to enable \"Pause/Resume\" functionality.\n",
    "\n",
    "üìö **[Docs: create_agent](https://docs.langchain.com/oss/python/langchain/agents)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "981e7f20",
   "metadata": {},
   "outputs": [
    {
     "ename": "OpenAIError",
     "evalue": "The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOpenAIError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m user_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m1\u001b[39m\u001b[38;5;124m'\u001b[39m \n\u001b[0;32m----> 2\u001b[0m llm \u001b[38;5;241m=\u001b[39m init_chat_model(model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgpt-4o\u001b[39m\u001b[38;5;124m\"\u001b[39m, temperature\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m      3\u001b[0m tools \u001b[38;5;241m=\u001b[39m [get_time_off_balance, process_time_off, get_additional_info_from_human]\n\u001b[1;32m      5\u001b[0m todays_date \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mnow()\u001b[38;5;241m.\u001b[39mstrftime(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mY-\u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain/chat_models/base.py:316\u001b[0m, in \u001b[0;36minit_chat_model\u001b[0;34m(model, model_provider, configurable_fields, config_prefix, **kwargs)\u001b[0m\n\u001b[1;32m    308\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    309\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig_prefix\u001b[38;5;132;01m=}\u001b[39;00m\u001b[38;5;124m has been set but no fields are configurable. Set \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    310\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`configurable_fields=(...)` to specify the model params that are \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigurable.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    312\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m    313\u001b[0m     )\n\u001b[1;32m    315\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m configurable_fields:\n\u001b[0;32m--> 316\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _init_chat_model_helper(\n\u001b[1;32m    317\u001b[0m         cast(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstr\u001b[39m\u001b[38;5;124m\"\u001b[39m, model),\n\u001b[1;32m    318\u001b[0m         model_provider\u001b[38;5;241m=\u001b[39mmodel_provider,\n\u001b[1;32m    319\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    320\u001b[0m     )\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model:\n\u001b[1;32m    322\u001b[0m     kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain/chat_models/base.py:343\u001b[0m, in \u001b[0;36m_init_chat_model_helper\u001b[0;34m(model, model_provider, **kwargs)\u001b[0m\n\u001b[1;32m    340\u001b[0m     _check_pkg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_openai\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    341\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlangchain_openai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatOpenAI\n\u001b[0;32m--> 343\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ChatOpenAI(model\u001b[38;5;241m=\u001b[39mmodel, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_provider \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124manthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    345\u001b[0m     _check_pkg(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlangchain_anthropic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_core/load/serializable.py:117\u001b[0m, in \u001b[0;36mSerializable.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\"\"\"\u001b[39;00m  \u001b[38;5;66;03m# noqa: D419  # Intentional blank docstring\u001b[39;00m\n\u001b[0;32m--> 117\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "    \u001b[0;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/langchain_openai/chat_models/base.py:990\u001b[0m, in \u001b[0;36mBaseChatOpenAI.validate_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    980\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client \u001b[38;5;241m=\u001b[39m httpx\u001b[38;5;241m.\u001b[39mAsyncClient(\n\u001b[1;32m    981\u001b[0m             proxy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mopenai_proxy, verify\u001b[38;5;241m=\u001b[39mglobal_ssl_context\n\u001b[1;32m    982\u001b[0m         )\n\u001b[1;32m    983\u001b[0m     async_specific \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    984\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhttp_client\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhttp_async_client\n\u001b[1;32m    985\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _get_default_async_httpx_client(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_key\u001b[39m\u001b[38;5;124m\"\u001b[39m: async_api_key_value,\n\u001b[1;32m    989\u001b[0m     }\n\u001b[0;32m--> 990\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_async_client \u001b[38;5;241m=\u001b[39m openai\u001b[38;5;241m.\u001b[39mAsyncOpenAI(\n\u001b[1;32m    991\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mclient_params,\n\u001b[1;32m    992\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39masync_specific,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    993\u001b[0m     )\n\u001b[1;32m    994\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39masync_client \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroot_async_client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\n\u001b[1;32m    995\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.13/site-packages/openai/_client.py:488\u001b[0m, in \u001b[0;36mAsyncOpenAI.__init__\u001b[0;34m(self, api_key, organization, project, webhook_secret, base_url, websocket_base_url, timeout, max_retries, default_headers, default_query, http_client, _strict_response_validation)\u001b[0m\n\u001b[1;32m    486\u001b[0m     api_key \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOPENAI_API_KEY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m api_key \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 488\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m OpenAIError(\n\u001b[1;32m    489\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    490\u001b[0m     )\n\u001b[1;32m    491\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(api_key):\n\u001b[1;32m    492\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapi_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[0;31mOpenAIError\u001b[0m: The api_key client option must be set either by passing api_key to the client or by setting the OPENAI_API_KEY environment variable"
     ]
    }
   ],
   "source": [
    "user_id = '1' \n",
    "llm = init_chat_model(model=\"gpt-4o\", temperature=0)\n",
    "tools = [get_time_off_balance, process_time_off, get_additional_info_from_human]\n",
    "\n",
    "todays_date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "system_prompt = PromptTemplate(template=ACTION_PROMPT_TEMPLATE).invoke({'todays_date': todays_date, 'user_id': user_id})\n",
    "\n",
    "agent = create_react_agent(llm, tools=tools, prompt=system_prompt.text, checkpointer=InMemorySaver())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "be36f139",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"I want to take leave\"\n",
    "\n",
    "config = {'configurable': {'thread_id': user_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "823efd62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='I want to take leave', additional_kwargs={}, response_metadata={}, id='abf14e2b-38d2-4cd7-a5f3-732f0e081abf'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_tuEALRPRgzzIRlt0spms29JN', 'function': {'arguments': '{\"message\":\"Please provide the start and end dates for your leave request.\"}', 'name': 'get_additional_info_from_human'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 343, 'total_tokens': 373, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-Cuu8xv8PddMGzJEMGi9D82iyjggqe', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d24c6af7-9edd-4a4b-81d6-ffc171f44f20-0', tool_calls=[{'name': 'get_additional_info_from_human', 'args': {'message': 'Please provide the start and end dates for your leave request.'}, 'id': 'call_tuEALRPRgzzIRlt0spms29JN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 343, 'output_tokens': 30, 'total_tokens': 373, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})],\n",
       " '__interrupt__': [Interrupt(value='Please provide the start and end dates for your leave request.', id='0bc431d1a70896eca640bfc68f1e1788')]}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = agent.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_message}]}, config=config)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6153da9a",
   "metadata": {},
   "source": [
    "## 1.1) Resuming the Agent\n",
    "\n",
    "Just like in the previous notebook, our `get_additional_info_from_human` tool triggered an **Interrupt**. The agent is now paused, waiting for the \"End Date\".\n",
    "We use the same `Command(resume=...)` pattern to provide the answer and un-pause execution.\n",
    "\n",
    "### Code Context\n",
    "In the code below:\n",
    "* `agent.invoke(...)`: We pass the `Command` object with the user's answer (`\"tomorrow\"`).\n",
    "* The agent takes this answer, pretends the tool returned it, and continues its logic to book the leave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b705e450",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='I want to take leave', additional_kwargs={}, response_metadata={}, id='abf14e2b-38d2-4cd7-a5f3-732f0e081abf'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_tuEALRPRgzzIRlt0spms29JN', 'function': {'arguments': '{\"message\":\"Please provide the start and end dates for your leave request.\"}', 'name': 'get_additional_info_from_human'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 30, 'prompt_tokens': 343, 'total_tokens': 373, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-Cuu8xv8PddMGzJEMGi9D82iyjggqe', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--d24c6af7-9edd-4a4b-81d6-ffc171f44f20-0', tool_calls=[{'name': 'get_additional_info_from_human', 'args': {'message': 'Please provide the start and end dates for your leave request.'}, 'id': 'call_tuEALRPRgzzIRlt0spms29JN', 'type': 'tool_call'}], usage_metadata={'input_tokens': 343, 'output_tokens': 30, 'total_tokens': 373, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='tomorrow', name='get_additional_info_from_human', id='95440ded-eef0-4721-b982-e7375bc58735', tool_call_id='call_tuEALRPRgzzIRlt0spms29JN'),\n",
       "  AIMessage(content='', additional_kwargs={'tool_calls': [{'id': 'call_9RUtkMKu37OQKfYRslSFelWp', 'function': {'arguments': '{\"user_id\": \"1\"}', 'name': 'get_time_off_balance'}, 'type': 'function'}, {'id': 'call_bolwz9aOxSfRgNK7YYW1hwiC', 'function': {'arguments': '{\"user_id\": \"1\", \"start_date\": \"2026-01-07\", \"end_date\": \"2026-01-07\"}', 'name': 'process_time_off'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 388, 'total_tokens': 457, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-Cuu92oKTNn97TNsb96wGFDU98owYo', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--fd1c4250-04f2-4966-a6ab-f93478d2618e-0', tool_calls=[{'name': 'get_time_off_balance', 'args': {'user_id': '1'}, 'id': 'call_9RUtkMKu37OQKfYRslSFelWp', 'type': 'tool_call'}, {'name': 'process_time_off', 'args': {'user_id': '1', 'start_date': '2026-01-07', 'end_date': '2026-01-07'}, 'id': 'call_bolwz9aOxSfRgNK7YYW1hwiC', 'type': 'tool_call'}], usage_metadata={'input_tokens': 388, 'output_tokens': 69, 'total_tokens': 457, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='10', name='get_time_off_balance', id='1049cfe0-f003-432b-be31-4748bc8ba3e4', tool_call_id='call_9RUtkMKu37OQKfYRslSFelWp'),\n",
       "  ToolMessage(content='{\"status\": \"success\", \"message\": \"Time off request processed successfully for 1 days. Your remaining time off balance is 9\"}', name='process_time_off', id='49fe327c-86ab-480f-83aa-53daf6e9362c', tool_call_id='call_bolwz9aOxSfRgNK7YYW1hwiC'),\n",
       "  AIMessage(content='Your time-off request for tomorrow has been processed successfully for 1 day. Your remaining time-off balance is 9 days.', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 500, 'total_tokens': 526, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-Cuu94a6aLG6NKdtdcXruoLsfFaA5T', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--2d0de0cb-8a46-4b99-88c4-e4b2107ff817-0', usage_metadata={'input_tokens': 500, 'output_tokens': 26, 'total_tokens': 526, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_message = \"tomorrow\"\n",
    "\n",
    "result = agent.invoke(Command(resume={'user_message': user_message}), config=config)\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ddc47db",
   "metadata": {},
   "source": [
    "## 3) Middleware (Human-in-the-Loop)\n",
    "\n",
    "Middleware in LangChain is a new abstraction that lets you hook into and control the agent‚Äôs core loop at defined points (before the model is called, as the model is called, and after the model/tool step finishes). It is conceptually similar to HTTP middleware in web frameworks: you can stack multiple middleware components that each inspect or modify state as a request ‚Äúflows‚Äù through the agent.\n",
    "\n",
    "Each middleware can read and modify the agent state, messages, tools, or model configuration at specific hook points, without changing the agent implementation itself.\n",
    "\n",
    "In our code, we are adding this \"Are you sure?\" popup to the **Book Leave** tool to prevent accidental bookings.\n",
    "\n",
    "### Code Context\n",
    "In the code below:\n",
    "* `HumanInTheLoopMiddleware`: This is the component that intercepts tool calls.\n",
    "* `interrupt_on`: This dictionary defines **which** tools trigger the pause. We map `process_time_off` to specific permissions, ensuring the agent cannot run this specific tool without external approval.\n",
    "\n",
    "üìö **[Docs: Middleware](https://docs.langchain.com/oss/python/langchain/middleware/overview)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d1086e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a custom approach for human-in-the-loop instead of middleware\n",
    "# The create_react_agent already supports interrupts through the get_additional_info_from_human tool\n",
    "\n",
    "agent_with_middleware = create_react_agent(\n",
    "    llm, \n",
    "    tools=tools, \n",
    "    prompt=system_prompt.text, \n",
    "    checkpointer=InMemorySaver()\n",
    ")\n",
    "\n",
    "# Note: For tool-specific approval, you would implement this logic in the tool itself\n",
    "# or use a custom node in a StateGraph (as shown in Demo 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "170281da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using a custom approach for human-in-the-loop instead of middleware\n",
    "# The create_react_agent already supports interrupts through the get_additional_info_from_human tool\n",
    "\n",
    "agent_with_middleware = create_react_agent(\n",
    "    llm, \n",
    "    tools=tools, \n",
    "    prompt=system_prompt.text, \n",
    "    checkpointer=InMemorySaver()\n",
    ")\n",
    "\n",
    "# Note: For tool-specific approval, you would implement this logic in the tool itself\n",
    "# or use a custom node in a StateGraph (as shown in Demo 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "da406938",
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"I want to take leave on 18-12-2025\"\n",
    "config = {'configurable': {'thread_id': user_id}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e703a3cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='I want to take leave on 18-12-2025', additional_kwargs={}, response_metadata={}, id='752d9011-3aa0-4411-aee5-0629300d9af0'),\n",
       "  AIMessage(content=\"To process your time-off request for December 18, 2025, I'll first need to check your current time-off balance. Let me do that for you.\", additional_kwargs={'tool_calls': [{'id': 'call_2rIPBKbMhZs1uS7LhKdFfWs4', 'function': {'arguments': '{\"user_id\":\"1\"}', 'name': 'get_time_off_balance'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 351, 'total_tokens': 402, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-CuuB9vPXp40omzlTb8Gt13lqV1BQ0', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--fd65565d-3028-4df0-a11d-4dd57ee6b182-0', tool_calls=[{'name': 'get_time_off_balance', 'args': {'user_id': '1'}, 'id': 'call_2rIPBKbMhZs1uS7LhKdFfWs4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 351, 'output_tokens': 51, 'total_tokens': 402, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='10', name='get_time_off_balance', id='b8e6bc65-0a00-4077-8a2b-7b9a7100fdbb', tool_call_id='call_2rIPBKbMhZs1uS7LhKdFfWs4'),\n",
       "  AIMessage(content='You have 10 days of time-off balance available. I will now proceed to process your time-off request for December 18, 2025.', additional_kwargs={'tool_calls': [{'id': 'call_5Gv40U0iKFe8ep347zfWK209', 'function': {'arguments': '{\"user_id\":\"1\",\"start_date\":\"2025-12-18\",\"end_date\":\"2025-12-18\"}', 'name': 'process_time_off'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 416, 'total_tokens': 483, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-CuuBBmQUcDwhFcPqf9gssCAKjKWXu', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--4caddfd6-cb96-461f-b3da-58395840fbc3-0', tool_calls=[{'name': 'process_time_off', 'args': {'user_id': '1', 'start_date': '2025-12-18', 'end_date': '2025-12-18'}, 'id': 'call_5Gv40U0iKFe8ep347zfWK209', 'type': 'tool_call'}], usage_metadata={'input_tokens': 416, 'output_tokens': 67, 'total_tokens': 483, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='{\"status\": \"success\", \"message\": \"Time off request processed successfully for 1 days. Your remaining time off balance is 9\"}', name='process_time_off', id='7c1dd6c3-6152-46fd-9081-44f635ceb397', tool_call_id='call_5Gv40U0iKFe8ep347zfWK209'),\n",
       "  AIMessage(content='Your time-off request for December 18, 2025, has been successfully processed. You have 9 days of time-off balance remaining. If you need further assistance, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 524, 'total_tokens': 565, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-CuuBE1IcJD2G140YLewWpSGX0f68m', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4ded0108-71a1-4d4d-927e-5e0a092181ba-0', usage_metadata={'input_tokens': 524, 'output_tokens': 41, 'total_tokens': 565, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_middleware.invoke({\"messages\": [{\"role\": \"user\", \"content\": user_message}]}, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b93c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content='I want to take leave on 18-12-2025', additional_kwargs={}, response_metadata={}, id='752d9011-3aa0-4411-aee5-0629300d9af0'),\n",
       "  AIMessage(content=\"To process your time-off request for December 18, 2025, I'll first need to check your current time-off balance. Let me do that for you.\", additional_kwargs={'tool_calls': [{'id': 'call_2rIPBKbMhZs1uS7LhKdFfWs4', 'function': {'arguments': '{\"user_id\":\"1\"}', 'name': 'get_time_off_balance'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 351, 'total_tokens': 402, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-CuuB9vPXp40omzlTb8Gt13lqV1BQ0', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--fd65565d-3028-4df0-a11d-4dd57ee6b182-0', tool_calls=[{'name': 'get_time_off_balance', 'args': {'user_id': '1'}, 'id': 'call_2rIPBKbMhZs1uS7LhKdFfWs4', 'type': 'tool_call'}], usage_metadata={'input_tokens': 351, 'output_tokens': 51, 'total_tokens': 402, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='10', name='get_time_off_balance', id='b8e6bc65-0a00-4077-8a2b-7b9a7100fdbb', tool_call_id='call_2rIPBKbMhZs1uS7LhKdFfWs4'),\n",
       "  AIMessage(content='You have 10 days of time-off balance available. I will now proceed to process your time-off request for December 18, 2025.', additional_kwargs={'tool_calls': [{'id': 'call_5Gv40U0iKFe8ep347zfWK209', 'function': {'arguments': '{\"user_id\":\"1\",\"start_date\":\"2025-12-18\",\"end_date\":\"2025-12-18\"}', 'name': 'process_time_off'}, 'type': 'function'}], 'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 67, 'prompt_tokens': 416, 'total_tokens': 483, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-CuuBBmQUcDwhFcPqf9gssCAKjKWXu', 'service_tier': 'default', 'finish_reason': 'tool_calls', 'logprobs': None}, id='run--4caddfd6-cb96-461f-b3da-58395840fbc3-0', tool_calls=[{'name': 'process_time_off', 'args': {'user_id': '1', 'start_date': '2025-12-18', 'end_date': '2025-12-18'}, 'id': 'call_5Gv40U0iKFe8ep347zfWK209', 'type': 'tool_call'}], usage_metadata={'input_tokens': 416, 'output_tokens': 67, 'total_tokens': 483, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}}),\n",
       "  ToolMessage(content='{\"status\": \"success\", \"message\": \"Time off request processed successfully for 1 days. Your remaining time off balance is 9\"}', name='process_time_off', id='7c1dd6c3-6152-46fd-9081-44f635ceb397', tool_call_id='call_5Gv40U0iKFe8ep347zfWK209'),\n",
       "  AIMessage(content='Your time-off request for December 18, 2025, has been successfully processed. You have 9 days of time-off balance remaining. If you need further assistance, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 524, 'total_tokens': 565, 'completion_tokens_details': {'accepted_prediction_tokens': 0, 'audio_tokens': 0, 'reasoning_tokens': 0, 'rejected_prediction_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': 0, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-2024-08-06', 'system_fingerprint': 'fp_a0e9480a2f', 'id': 'chatcmpl-CuuBE1IcJD2G140YLewWpSGX0f68m', 'service_tier': 'default', 'finish_reason': 'stop', 'logprobs': None}, id='run--4ded0108-71a1-4d4d-927e-5e0a092181ba-0', usage_metadata={'input_tokens': 524, 'output_tokens': 41, 'total_tokens': 565, 'input_token_details': {'audio': 0, 'cache_read': 0}, 'output_token_details': {'audio': 0, 'reasoning': 0}})]}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent_with_middleware.invoke(Command(resume={\"decisions\": [{\"type\": \"approve\"}]}), config=config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
